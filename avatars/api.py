# This file has been generated - DO NOT MODIFY
<<<<<<< HEAD
# API Version : 1.1.0-8d3fdf018d62c07cb630d4d02e690cf75f8411ea
||||||| parent of 37080c14 (chore: generate code to handle large files using filehandle)
# API Version : 0.5.24-0e261dc021c2d7f16d2f8332463a86fa90360196
=======
# API Version : 0.5.24-9abafdba2efbeee858fcabff049b56b18a218e9e
>>>>>>> 37080c14 (chore: generate code to handle large files using filehandle)


import io
import itertools
import logging
import os
import shutil
import tempfile
import time
import warnings
from contextlib import ExitStack
from copy import copy
from io import BytesIO, StringIO
from pathlib import Path
from typing import (
    IO,
    TYPE_CHECKING,
    Any,
    AnyStr,
    BinaryIO,
    Callable,
    Dict,
    Iterator,
    List,
    Literal,
    NoReturn,
    Optional,
    Protocol,
    Sequence,
    TextIO,
    Tuple,
    TypeVar,
    Union,
    cast,
    overload,
)
from uuid import UUID

import httpx
import numpy as np
import pandas as pd
import pyarrow
from pyarrow import parquet as pq
from pydantic import BaseModel
from toolz.dicttoolz import dissoc

from avatars.api_autogenerated import Auth as _Auth
from avatars.api_autogenerated import Compatibility as _Compatibility
from avatars.api_autogenerated import Datasets as _Datasets
from avatars.api_autogenerated import Health as _Health
from avatars.api_autogenerated import Jobs as _Jobs
from avatars.api_autogenerated import Metrics as _Metrics
from avatars.api_autogenerated import Reports as _Reports
from avatars.api_autogenerated import Stats as _Stats
from avatars.api_autogenerated import Users as _Users
from avatars.client import (
    build_request,
    handle_response_failure,
    handle_response_stream,
    send_request,
)
from avatars.constants import DEFAULT_TIMEOUT
from avatars.exceptions import FileTooLarge, InvalidFileType, Timeout
from avatars.models import (
    AvatarizationBatchJob,
    AvatarizationBatchJobCreate,
    AvatarizationBatchResult,
    AvatarizationJob,
    AvatarizationJobCreate,
    AvatarizationMultiTableJob,
    AvatarizationMultiTableJobCreate,
    AvatarizationPipelineCreate,
    AvatarizationPipelineResult,
    AvatarizationWithTimeSeriesJob,
    AvatarizationWithTimeSeriesJobCreate,
    ClusterStats,
    ColumnDetail,
    ColumnType,
    CompatibilityResponse,
    Contributions,
    CreateDataset,
    CreateUser,
    Dataset,
    ExplainedVariance,
    FileType,
    ForgottenPasswordRequest,
    GenericJob,
    JobStatus,
    Login,
    LoginResponse,
    PatchDataset,
    PrivacyMetrics,
    PrivacyMetricsBatchJob,
    PrivacyMetricsBatchJobCreate,
    PrivacyMetricsGeolocationJob,
    PrivacyMetricsGeolocationJobCreate,
    PrivacyMetricsJob,
    PrivacyMetricsJobCreate,
    PrivacyMetricsMultiTableJob,
    PrivacyMetricsMultiTableJobCreate,
    PrivacyMetricsParameters,
    PrivacyMetricsWithTimeSeriesJob,
    PrivacyMetricsWithTimeSeriesJobCreate,
    Processor,
    Projections,
    Report,
    ReportCreate,
    ReportFromBatchCreate,
    ReportFromDataCreate,
    ReportGeolocationPrivacyCreate,
    ResetPasswordRequest,
    SignalMetrics,
    SignalMetricsBatchJob,
    SignalMetricsBatchJobCreate,
    SignalMetricsJob,
    SignalMetricsJobCreate,
    SignalMetricsParameters,
    SignalMetricsWithTimeSeriesJob,
    SignalMetricsWithTimeSeriesJobCreate,
    User,
)

if TYPE_CHECKING:
    from avatars.client import ApiClient
    from avatars._typing import FileLikeInterface, HttpxFile

from avatars._typing import is_file_like

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())
DEFAULT_RETRY_TIMEOUT = 60
DEFAULT_TIMEOUT = 5
MAX_ROWS_PER_FILE = 1_000_000
MAX_BYTES_PER_FILE = 100 * 1024 * 1024  # 100 MB

PARQUET_MAGIC_BYTES = b"PAR1"

T = TypeVar("T")


def to_column_type(s: str) -> ColumnType:
    if "float" in s:
        return ColumnType.float
    if "int" in s:
        return ColumnType.int
    if "bool" in s:
        return ColumnType.bool
    if "datetime" in s:
        return ColumnType.datetime
    if "object" in s or s == "category" or s == "str":
        return ColumnType.category
    raise TypeError(f"Unknown column type: '{s}'")


def from_column_type(s: ColumnType) -> str:
    # We don't want to use 'category' here, as we store it as 'object' in the API
    if s == ColumnType.category:
        return "object"
    else:
        return str(s.value)

    raise TypeError(f"Unknown column type: '{s}'")


class Auth:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def login(
        self,
        request: Login,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> LoginResponse:
        """Login the user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).login(*args, **kwargs)

    def forgotten_password(
        self,
        request: ForgottenPasswordRequest,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).forgotten_password(*args, **kwargs)

    def reset_password(
        self,
        request: ResetPasswordRequest,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).reset_password(*args, **kwargs)


class Compatibility:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def is_client_compatible(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> CompatibilityResponse:
        """Verify if the client is compatible with the API."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Compatibility(self.client).is_client_compatible(*args, **kwargs)


class Datasets:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def create_dataset_from_stream(
        self,
        request: Optional[
            Union["FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        name: Optional[str] = None,
        source: Optional[
            Union[str, "FileLikeInterface[bytes]"]
        ] = None,  # optional because we still have to support the old way # TODO: Remove once deprecated
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Create a dataset by streaming chunks of the dataset.

        DEPRECATED: Please use create_dataset instead.
        """

        warnings.warn(
            DeprecationWarning(
                "create_dataset_from_stream is deprecated. Use create_dataset instead."
            )
        )

        _source: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = (request or source)
        return self.create_dataset(source=_source, name=name, timeout=timeout)  # type: ignore[arg-type]

    def create_dataset(
        self,
        request: Optional[
            Union["FileLikeInterface[str]", "FileLikeInterface[bytes]"]
        ] = None,  # TODO: Remove once deprecated
        name: Optional[str] = None,
        source: Optional[
            Union[
                str,
                list[str],
                "FileLikeInterface[bytes]",
            ]
        ] = None,  # optional because we still have to support the old way
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Create a dataset from file upload."""

        if request:
            warnings.warn(
                DeprecationWarning("request is deprecated. Use file instead.")
            )

        if request is not None and source is not None:
            raise ValueError("You cannot pass both request and source.")

        if request is None and source is None:
            raise ValueError("You need to pass in a source.")

        _source = request or source

        if _source is None:
            raise ValueError("You need to pass in a source.")

        with ExitStack() as stack:
            file_arguments = self._create_httpx_file_argument(_source, stack)

            kwargs = {
                "method": "post",
                "url": f"/datasets/stream",
                "timeout": timeout,
                "file": file_arguments,
                "params": dict(
                    name=name,
                ),
            }

            result = self.client.request(**kwargs)  # type: ignore[arg-type]
        return Dataset(**result)

    def download_dataset_as_stream(
        self,
        id: str,
        destination: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        *,
        filetype: Optional[FileType] = None,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> BytesIO:
        """Download a dataset by streaming chunks of it.

        DEPRECATED: Please use download_dataset instead.
        """
        warnings.warn(
            DeprecationWarning(
                "download_dataset_as_stream is deprecated. Use download_dataset instead."
            )
        )

        # Ignoring return type because download_dataset's logic makes sure
        # that the return type will be BytesIO
        # No point in going through the hassle of using typing.overload for something
        # that will be removed soon
        return self.download_dataset(  # type: ignore[return-value]
            id,
            destination=destination,
            timeout=timeout,
            filetype=filetype,
            from_download_as_stream=True,
        )

    def download_dataset(
        self,
        id: str,
        destination: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        filetype: Optional[FileType] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        from_download_as_stream: bool = False,  # TODO: Remove once deprecated
    ) -> Optional[Union[BytesIO, str]]:
        """Download a dataset."""

        CHUNK_SIZE = 1024 * 1024  # 1MB
        # Download dataset metadata
        dataset_info = self.client.datasets.get_dataset(id, timeout=timeout)

        if destination is None and not from_download_as_stream:
            # Download as CSV was the old return value when using download_dataset
            filetype = filetype or FileType.csv

        elif destination is None and from_download_as_stream:
            # Download as the filetype of the dataset on the server was the old
            # return value when using download_dataset_as_stream
            filetype = filetype or dataset_info.filetype

        kwargs = {
            "method": "get",
            "url": f"/datasets/{id}/download/stream",
            "timeout": timeout,
            "should_stream": True,
            "params": {
                "filetype": filetype,
            },
        }

        invalid_destination_error = TypeError(
            f"Expected destination to be a string or a buffer, got {type(destination)} instead"
        )

        with tempfile.TemporaryDirectory() as download_dir:
            download_path = Path(download_dir) / "downloaded"
            download_path.touch()

            with self.client._get_http_client() as http_client:
                request = build_request(
                    http_client,
                    **dissoc(kwargs, "timeout", "should_stream"),
                    headers=self.client._headers,
                )
                response = send_request(http_client, request, should_stream=True)

                if not response.is_success:
                    handle_response_failure(response, should_stream=True)

                # We first write the whole response to disk, to avoid memory problems
                with open(download_path, "wb") as file:
                    handle_response_stream(response, destination=file)

            # The server returns the data in the same type as the filetype the dataset was saved in,
            # if no filetype is specified in download_dataset
            downloaded_filetype = filetype or dataset_info.filetype
            parquet_dir = Path(download_dir) / "parquet"
            if downloaded_filetype is FileType.parquet:
                # If it's a parquet file, we have to open the file and split it into multiple files
                # as the server sends us parquet fragments, which we saved in a single file upon
                # receiving the stream.
                with open(download_path, "rb") as input_file:
                    count = 0
                    parquet_dir = Path(download_dir) / "parquet"
                    parquet_dir.mkdir()
                    # Read the file in chunks of 1MB
                    while content := input_file.read(CHUNK_SIZE):
                        self._split_parquet_file(content, parquet_dir, count)

                # Collect all filenames and store them into a single parquet file
                concatenated_filename = Path(download_dir) / "concatenated.parquet"
                filenames = self._collect_filenames(parquet_dir)
                schema = pyarrow.parquet.ParquetFile(filenames[0]).schema_arrow
                with pyarrow.parquet.ParquetWriter(
                    str(concatenated_filename), schema=schema
                ) as writer:
                    for filename in filenames:
                        writer.write_table(
                            pyarrow.parquet.read_table(filename, schema=schema)
                        )

                if isinstance(destination, str):
                    # We simply 'mv' the concatenated filename to the destination
                    # desired by the user
                    shutil.move(str(concatenated_filename), destination)
                    return None

                if destination and not is_file_like(destination):
                    raise invalid_destination_error

                output_buffer = BytesIO() if destination is None else destination

                try:
                    # We don't know the type of the buffer, so we try to write the bytes to it
                    with open(concatenated_filename, "rb") as file:
                        while as_bytes := file.read(CHUNK_SIZE):
                            # error: Argument 1 to "write" of "SupportsWrite"
                            # has incompatible type "bytes"; expected "str"
                            output_buffer.write(as_bytes)  # type: ignore[arg-type]

                except TypeError as e:
                    message = str(e.args[0])
                    messages_inducing_raise = [
                        "string argument expected",
                        "write() argument must be str, not bytes",
                    ]
                    if any(m in message for m in messages_inducing_raise):
                        raise InvalidFileType(
                            "Can't download parquet files as string. Please use a different destination."
                        )
                    raise e

                output_buffer.seek(0)
                # Destination = None is the deprecated case
                # TODO: Remove once deprecated
                if destination is None:
                    warnings.warn(
                        DeprecationWarning(
                            "Please specify the destination argument. The return type will change in the future."
                        )
                    )

                    output_buffer.seek(0)
                    if from_download_as_stream:
                        # Old return value when using download_dataset_as_stream

                        # Old return value when using download_dataset_as_stream
                        # Incompatible return value type (got
                        # "Union[BytesIO, FileLikeInterface[bytes], FileLikeInterface[str]]", expected
                        # "Union[BytesIO, str, None]")  [return-value]

                        return output_buffer  # type: ignore[return-value]
                    else:
                        # Old return value when using download_dataset (with Filetype.csv as default)
                        raise InvalidFileType(
                            "Can't download parquet files as string. Please use a different destination."
                        )

                # We return the buffer, as the user provided it
                return None
            else:
                # Destination = None is the deprecated case
                # TODO: Remove once deprecated
                if destination is None:
                    warnings.warn(
                        DeprecationWarning(
                            "Please specify the destination argument. The return type will change in the future."
                        )
                    )
                    buffer = BytesIO()
                    with open(download_path, "rb") as file:
                        buffer.write(file.read())
                    buffer.seek(0)

                    if from_download_as_stream:
                        # Old return value when using download_dataset_as_stream
                        return buffer
                    else:
                        # Old return value when using download_dataset (with Filetype.csv as default)
                        return buffer.read().decode()

                if isinstance(destination, str):
                    # We simply 'mv' the concatenated filename to the destination
                    # desired by the user
                    shutil.move(str(download_path), destination)
                    return None

                if is_file_like(destination):
                    # User wants to save the file to a buffer he provided

                    # We don't know the type of the buffer, so we try to write the text to it
                    # and if it fails, we break and re-open the file as bytes
                    is_string_buffer = True
                    with open(download_path, "r") as file:
                        while as_string := file.read(CHUNK_SIZE):
                            try:
                                destination.write(as_string)
                            except TypeError as e:
                                if "a bytes-like object is required" in str(e.args[0]):
                                    is_string_buffer = False
                                    break
                                else:
                                    raise e

                    if is_string_buffer is False:
                        with open(download_path, "rb") as file:
                            while as_bytes := file.read(CHUNK_SIZE):
                                # error: Argument 1 to "write" of "SupportsWrite"
                                # has incompatible type "bytes";
                                # expected "str"  [arg-type]
                                destination.write(as_bytes)  # type: ignore[arg-type]

                    destination.seek(0)
                    return None

                raise invalid_destination_error

    def _collect_filenames(self, path: Path) -> List[Path]:
        """
        Collects and sorts the filenames in a given directory and its subdirectories.

        This function collects all the filenames in the specified path and its subdirectories.
        It then sorts the filenames based on the number at the end of the filename. The sorting
        places 0 before 1, 2, etc., rather than 1, 10, 11, 12, etc. This is to ensure correct
        order during upload and concatenation when downloaded.

        Parameters
        ----------
        path
            The directory path where the files are located.

        Returns
        -------
        List[Path]
            The list of sorted filenames.
        """

        filenames = [f for f in path.glob("**/*") if f.is_file()]

        # Sort the file names to ensure correct order during upload and concatenation when downloaded.
        # The files are named with a number at the end, so sorting is based on that number.
        # Note that the sorting places 0 before 1, 2, etc., rather than 1, 10, 11, 12, etc.
        # Example filename: /tmp/tmp1f1z2qk2/33e74308ee414f94885d99236d10faf7-0.parquet

        filenames.sort(key=lambda x: int(x.name.split("-")[1].split(".")[0]))

        return filenames

    def _create_httpx_file_argument(
        self,
        source: Union[
            str,
            list[str],
            "FileLikeInterface[bytes]",
            "FileLikeInterface[str]",  # TODO: remove that possibility once deprecated
        ],
        stack: ExitStack,
    ) -> List["HttpxFile"]:
        if source and is_file_like(source):
            # User provided file like object.
            # It is his responsibility to close it.

            # Get size of source, so we can check if it's too large
            # and split it into multiple files

            source_needs_to_be_split = False
            if source.read(MAX_BYTES_PER_FILE) and source.read(1):
                # There are more bytes to read, so the file is too large
                # and we have to split it into multiple files
                source_needs_to_be_split = True
            source.seek(0)

            if not source_needs_to_be_split:
                # We try reading the file to check if it's binary or not
                # HTTPX handles binary files only
                single_char = source.read(1)

                if isinstance(single_char, bytes):
                    # undo previous read operation
                    # can only be done on binary file handles
                    source.seek(-1, os.SEEK_CUR)

                    # File is binary, we can upload it as is
                    return [("file", source)]

                # TODO: When deprecating, raise an error instead of a warning
                warnings.warn(
                    DeprecationWarning(
                        "You are trying to upload a text file. This is deprecated. "
                        "Please open the file in binary mode."
                    )
                )

                # We create a new temporary file, write the content of the source file to it
                # and then upload the temporary file, but this time opened in binary mode (default)
                temporary_file = stack.enter_context(tempfile.NamedTemporaryFile())
                temporary_file.write(single_char.encode())
                temporary_file.write(source.read().encode())
                temporary_file.seek(0)

                return [("file", temporary_file)]

            # TODO: Split the file without loading it all into memory.
            try:
                df = pd.read_parquet(source, engine="pyarrow")
            except (TypeError, pyarrow.lib.ArrowInvalid) as e:
                expected_messages = [
                    "binary file expected",
                    "this is not a parquet file",
                ]
                if any(m in str(e.args[0]) for m in expected_messages):
                    source.seek(0)
                    df = pd.read_csv(source)

                else:
                    raise e

            temp_dir = stack.enter_context(tempfile.TemporaryDirectory())
            # Write the table into multiple parquet files
            pq.write_to_dataset(
                pyarrow.Table.from_pandas(df),
                root_path=temp_dir,
                max_rows_per_file=MAX_ROWS_PER_FILE,
                row_group_size=MAX_ROWS_PER_FILE,
                max_rows_per_group=MAX_ROWS_PER_FILE,
            )
            # Create a list, so that the if statement further down can upload the files
            source = list(map(str, self._collect_filenames(Path(temp_dir))))

        if isinstance(source, str):
            source = [source]

        if isinstance(source, list):
            # List of files to upload as one dataset.
            # This is especially useful for large parquet files.
            return [
                ("file", stack.enter_context(open(file_path, "rb")))
                for file_path in source
            ]

        raise TypeError(
            f"Expected source to be a string or a buffer, got {type(source)} instead."
        )

    def _split_parquet_file(self, content: bytes, parquet_dir: Path, count: int) -> int:
        """
        Splits a parquet file into multiple smaller parquet files.

        This function recursively splits the input bytes content into separate parquet files.
        The split is done based on the PARQUET_MAGIC_BYTES delimiter. The resulting files are
        stored in the specified directory and are named as "file-{count}.parquet".

        Parameters
        ----------
        content
            The content of the parquet file to be split.
        parquet_dir
            The directory where the split parquet files will be stored.
        count
            The starting count for the file naming.

        Returns
        -------
        int
            The count of the last file written. This can be used to continue writing files
            with sequential names in subsequent function calls.

        """

        delimiter = PARQUET_MAGIC_BYTES + PARQUET_MAGIC_BYTES
        if delimiter in content:
            all = content.split(delimiter, 1)
            if len(all) > 1:
                before, after = all
                after = PARQUET_MAGIC_BYTES + after
                before = before + PARQUET_MAGIC_BYTES
            else:
                before = all[0]
                after = b""
        else:
            before = content
            after = b""

        current_file = parquet_dir / f"file-{count}.parquet"
        current_file.touch(exist_ok=True)
        with open(current_file, "wb") as f:
            f.write(before)

        if after:
            count = self._split_parquet_file(after, parquet_dir, count + 1)

        return count  # to be able to continue on the same file

    def find_all_datasets_by_user(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[Dataset]:
        """List all datasets of the current_user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Datasets(self.client).find_all_datasets_by_user(*args, **kwargs)

    def get_dataset(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Get a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).get_dataset(*args, **kwargs)

    def patch_dataset(
        self,
        request: PatchDataset,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Modify a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
            id,
        ]

        return _Datasets(self.client).patch_dataset(*args, **kwargs)

    def analyze_dataset(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Start the analysis of a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).analyze_dataset(*args, **kwargs)

    def get_dataset_correlations(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Get a dataset's correlations."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).get_dataset_correlations(*args, **kwargs)


class Health:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_root(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify server health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_root(*args, **kwargs)

    def get_health(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify server health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_health(*args, **kwargs)

    def get_health_db(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify connection to the db health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_health_db(*args, **kwargs)


class Jobs:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def find_all_jobs_by_user(
        self,
        nb_days: Optional[int] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[GenericJob]:
        """Retrieve all jobs executed by the current user.

        Jobs are filtered by execution date, by default only the last 5 days are displayed,
        a parameter can be provided to go further back in time.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            nb_days,
        ]

        return _Jobs(self.client).find_all_jobs_by_user(*args, **kwargs)

    def create_full_avatarization_job(
        self,
        request: AvatarizationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Create an avatarization job, then calculate metrics."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_full_avatarization_job(*args, **kwargs)

    def cancel_job(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> GenericJob:
        """Cancel any kind of job.

        If the job hadn't been started yet, revoke it.
        If the job is ongoing, gently kill it.
        If the job is done, do nothing.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).cancel_job(*args, **kwargs)

    def create_avatarization_job(
        self,
        request: AvatarizationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Create an avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_job(*args, **kwargs)

    def create_avatarization_batch_job(
        self,
        request: AvatarizationBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationBatchJob:
        """Create an avatarization batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_batch_job(*args, **kwargs)

    def create_avatarization_with_time_series_job(
        self,
        request: AvatarizationWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationWithTimeSeriesJob:
        """Create an avatarization with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_with_time_series_job(
            *args, **kwargs
        )

    def create_avatarization_multi_table_job(
        self,
        request: AvatarizationMultiTableJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationMultiTableJob:
        """Create an avatarization for relational data."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_multi_table_job(*args, **kwargs)

    def create_signal_metrics_job(
        self,
        request: SignalMetricsJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsJob:
        """Create a signal metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_job(*args, **kwargs)

    def create_privacy_metrics_job(
        self,
        request: PrivacyMetricsJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsJob:
        """Create a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_job(*args, **kwargs)

    def create_privacy_metrics_batch_job(
        self,
        request: PrivacyMetricsBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsBatchJob:
        """Create a privacy metrics batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_batch_job(*args, **kwargs)

    def create_privacy_metrics_time_series_job(
        self,
        request: PrivacyMetricsWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsWithTimeSeriesJob:
        """Create a privacy metrics with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_time_series_job(
            *args, **kwargs
        )

    def create_signal_metrics_time_series_job(
        self,
        request: SignalMetricsWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsWithTimeSeriesJob:
        """Create a signal metrics with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_time_series_job(*args, **kwargs)

    def create_signal_metrics_batch_job(
        self,
        request: SignalMetricsBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsBatchJob:
        """Create a signal metrics batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_batch_job(*args, **kwargs)

    def create_privacy_metrics_multi_table_job(
        self,
        request: PrivacyMetricsMultiTableJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsMultiTableJob:
        """Create a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_multi_table_job(
            *args, **kwargs
        )

    def create_privacy_metrics_geolocation_job(
        self,
        request: PrivacyMetricsGeolocationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsGeolocationJob:
        """Create a geolocation privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_geolocation_job(
            *args, **kwargs
        )

    def get_privacy_metrics_geolocation_job(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsGeolocationJob:
        """Get a geolocation privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_geolocation_job(*args, **kwargs)

    def get_avatarization_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Get an avatarization job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_job(*args, **kwargs)

    def get_avatarization_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationBatchJob:
        """Get an avatarization batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_batch_job(*args, **kwargs)

    def get_avatarization_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationWithTimeSeriesJob:
        """Get an avatarization time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_time_series_job(*args, **kwargs)

    def get_avatarization_multi_table_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationMultiTableJob:
        """Get a multi table avatarization job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_multi_table_job(*args, **kwargs)

    def get_signal_metrics(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsJob:
        """Get a signal metrics job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics(*args, **kwargs)

    def get_signal_metrics_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsBatchJob:
        """Get a signal metrics batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics_batch_job(*args, **kwargs)

    def get_signal_metrics_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsWithTimeSeriesJob:
        """Get a signal metrics time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics_time_series_job(*args, **kwargs)

    def get_privacy_metrics(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsJob:
        """Get a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics(*args, **kwargs)

    def get_privacy_metrics_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsBatchJob:
        """Get a privacy metrics batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_batch_job(*args, **kwargs)

    def get_privacy_metrics_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsWithTimeSeriesJob:
        """Get a privacy metrics time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_time_series_job(*args, **kwargs)

    def get_privacy_metrics_multi_table_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsMultiTableJob:
        """Get a privacy metrics multi table job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_multi_table_job(*args, **kwargs)


class Metrics:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_job_projections(
        self,
        job_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Projections:
        """Get the projections of records and avatars in 3D.

        See https://saiph.readthedocs.io/en/latest/ for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
        ]

        return _Metrics(self.client).get_job_projections(*args, **kwargs)

    def get_variable_contributions(
        self,
        job_id: str,
        dataset_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Contributions:
        """Get the contributions of the dataset variables within the fitted space.

        See https://saiph.readthedocs.io/en/latest for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
            dataset_id,
        ]

        return _Metrics(self.client).get_variable_contributions(*args, **kwargs)

    def get_explained_variance(
        self,
        job_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> ExplainedVariance:
        """Get the explained variance of records.

        See https://saiph.readthedocs.io/en/latest/ for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
        ]

        return _Metrics(self.client).get_explained_variance(*args, **kwargs)


class Reports:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def create_report(
        self,
        request: ReportCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report(*args, **kwargs)

    def get_report(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Reports(self.client).get_report(*args, **kwargs)

    def download_report(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Download a report."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Reports(self.client).download_report(*args, **kwargs)

    def create_report_from_data(
        self,
        request: ReportFromDataCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report without avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report_from_data(*args, **kwargs)

    def create_report_from_batch(
        self,
        request: ReportFromBatchCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report from batch job identifiers.

        The report will be generated with the worst privacy_metrics and the mean signal_metrics.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report_from_batch(*args, **kwargs)

    def create_geolocation_privacy_report(
        self,
        request: ReportGeolocationPrivacyCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report without avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_geolocation_privacy_report(*args, **kwargs)


class Stats:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_cluster_stats(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> ClusterStats:
        """Get insights into the cluster's usage."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Stats(self.client).get_cluster_stats(*args, **kwargs)


class Users:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def find_users(
        self,
        email: Optional[str] = None,
        username: Optional[str] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[User]:
        """Get users, optionally filtering them by username or email.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            email,
            username,
        ]

        return _Users(self.client).find_users(*args, **kwargs)

    def create_user(
        self,
        request: CreateUser,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Create a user.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Users(self.client).create_user(*args, **kwargs)

    def get_me(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Get my own user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Users(self.client).get_me(*args, **kwargs)

    def get_user(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Get a user by id.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Users(self.client).get_user(*args, **kwargs)


class PandasIntegration:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def upload_dataframe(
        self,
        request: "pd.DataFrame",
        name: Optional[str] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        identifier_variables: List[str] = [],
        **kwargs: Dict[str, Any],  # to collect should_stream
    ) -> Dataset:
        if "should_stream" in kwargs:
            warnings.warn(
                "The `should_stream` parameter is deprecated and will be removed in a future version. "
                "All uploads are now streamed by default.",
                DeprecationWarning,
            )

        for col in request.columns:
            if pd.api.types.infer_dtype(request[col], skipna=True) in (
                "mixed-integer",
                "mixed",
            ):
                raise ValueError(
                    f"Expected column '{col}' should have either str or numeric values."
                    " Consider harmonizing columns prior to upload."
                )

        df_types = request.dtypes
        table = pyarrow.Table.from_pandas(request)
        del request

        # Create a temporary directory to store the split parquet files
        with tempfile.TemporaryDirectory() as temp_dir:
            # Write the table into multiple parquet files
            pq.write_to_dataset(
                table,
                root_path=temp_dir,
                max_rows_per_file=MAX_ROWS_PER_FILE,
                row_group_size=MAX_ROWS_PER_FILE,
                max_rows_per_group=MAX_ROWS_PER_FILE,
            )

            dataset = self.client.datasets.create_dataset(
                source=list(
                    map(str, Datasets(self.client)._collect_filenames(Path(temp_dir)))
                ),
                name=name,
                timeout=timeout,
            )

        columns = []
        for index, dtype in zip(df_types.index, df_types):
            if index in identifier_variables:
                column_detail = ColumnDetail(type=ColumnType.identifier, label=index)
            else:
                column_detail = ColumnDetail(
                    type=to_column_type(str(dtype)), label=index
                )
            columns.append(column_detail)

        dataset = self.client.datasets.patch_dataset(
            id=str(dataset.id),
            request=PatchDataset(columns=columns),
        )
        return dataset

    def download_dataframe(
        self,
        id: str,
        *,
        filetype: Optional[FileType] = None,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        **kwargs: Any,  # to collect should_stream
    ) -> pd.DataFrame:
        if "should_stream" in kwargs:
            warnings.warn(
                "The `should_stream` parameter is deprecated and will be removed in a future version. "
                "All uploads are now streamed by default.",
                DeprecationWarning,
            )

        dataset = self.client.datasets.get_dataset(id, timeout=timeout)

        _filetype = filetype or dataset.filetype
        with tempfile.TemporaryDirectory() as download_dir:
            path = Path(download_dir) / "file"
            Datasets(client=self.client).download_dataset(
                id, str(path), timeout=timeout, filetype=_filetype
            )
            if _filetype is FileType.parquet:
                df = pd.read_parquet(path, engine="pyarrow")
            elif _filetype is FileType.csv:
                df = pd.read_csv(path)
            else:
                raise ValueError(f"Unsupported filetype: {_filetype}")

        # We apply datetime columns separately as 'datetime' is not a valid pandas dtype
<<<<<<< HEAD
        dtypes = {c.label: c.type.value for c in dataset_info.columns or {}}
||||||| parent of f3ca9778 (chore: generate code)
        dtypes = {
            c.label: c.type.value
            for c in dataset_info.columns or {}
            if c.type is not ColumnType.identifier
        }
=======
        dtypes = {
            c.label: from_column_type(c.type)
            for c in dataset.columns or {}
            if c.type is not ColumnType.identifier
        }
>>>>>>> f3ca9778 (chore: generate code)
        datetime_columns = [
            label for label, type in dtypes.items() if type == ColumnType.datetime.value
        ]

        # Remove datetime columns
        for label in list(dtypes.keys()):
            if label in datetime_columns:
                dtypes.pop(label, None)

        for name, dtype in dtypes.items():
            df[name] = df[name].astype(dtype)

        df[datetime_columns] = df[datetime_columns].astype("datetime64[ns]")

        return df


class Pipelines:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def avatarization_pipeline_with_processors(
        self,
        request: AvatarizationPipelineCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationPipelineResult:
        """Create a Pipeline of avatarization with processors."""

        # Upload the dataset if needed
        # User can either specify the dataset he already uploaded or let this pipeline upload it
        df = request.df.copy()
        original_dataset_id = (
            request.avatarization_job_create.parameters.dataset_id
            or self.client.pandas_integration.upload_dataframe(df, timeout=timeout).id
        )

        # Pre process the dataframe and upload it
        processors = request.processors
        for p in processors:
            df = p.preprocess(df)
        dataset = self.client.pandas_integration.upload_dataframe(
            df, timeout=per_request_timeout
        )

        # Avatarize the uploaded dataframe
        request.avatarization_job_create.parameters.dataset_id = dataset.id
        avatarization_job = self.client.jobs.create_avatarization_job(
            request.avatarization_job_create
        )
        print(f"launching avatarization job with id={avatarization_job.id}")

        avatarization_job = self.client.jobs.get_avatarization_job(
            str(avatarization_job.id),
            timeout=timeout,
            per_request_timeout=per_request_timeout,
        )
        if avatarization_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the avatarization job: {avatarization_job.error_message}"
            )

        if (
            avatarization_job.status == JobStatus.pending
            or not avatarization_job.result
        ):
            raise Timeout(
                f"The avatarization job '{avatarization_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        # Download the dataframe, postprocess it and upload the new dataframe
        sensitive_unshuffled_avatars = (
            self.client.pandas_integration.download_dataframe(
                str(avatarization_job.result.sensitive_unshuffled_avatars_datasets.id),
                timeout=timeout,
            )
        )
        for p in reversed(processors):
            sensitive_unshuffled_avatars = p.postprocess(
                request.df, sensitive_unshuffled_avatars
            )
        unshuffled_dataset = self.client.pandas_integration.upload_dataframe(
            sensitive_unshuffled_avatars, timeout=timeout
        )

        # Calculate privacy metrics on the post processed dataset vs the original one
        privacy_job = self.client.jobs.create_privacy_metrics_job(
            PrivacyMetricsJobCreate(
                parameters=PrivacyMetricsParameters(
                    original_id=original_dataset_id,
                    unshuffled_avatars_id=unshuffled_dataset.id,
                )
            ),
            timeout=per_request_timeout,
        )
        print(f"launching privacy metrics job with id={privacy_job.id}")

        # Calculate signal metrics
        signal_job = self.client.jobs.create_signal_metrics_job(
            SignalMetricsJobCreate(
                parameters=SignalMetricsParameters(
                    original_id=original_dataset_id, avatars_id=unshuffled_dataset.id
                )
            ),
            timeout=per_request_timeout,
        )
        print(f"launching signal metrics job with id={signal_job.id}")

        # Get the job results
        signal_job = self.client.jobs.get_signal_metrics(
            str(signal_job.id), timeout=timeout, per_request_timeout=per_request_timeout
        )
        privacy_job = self.client.jobs.get_privacy_metrics(
            str(privacy_job.id),
            timeout=timeout,
            per_request_timeout=per_request_timeout,
        )
        if signal_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the signal metrics job: {signal_job.error_message}"
            )
        if privacy_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the privacy metrics job: {privacy_job.error_message}"
            )

        if signal_job.status == JobStatus.pending or not signal_job.result:
            raise Timeout(
                f"The signal metrics job '{signal_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        if privacy_job.status == JobStatus.pending or not privacy_job.result:
            raise Timeout(
                f"The privacy metrics job '{privacy_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        # Shuffle sensitive_unshuffled_avatars for security reasons
        random_gen = np.random.default_rng()
        map = random_gen.permutation(sensitive_unshuffled_avatars.index.values).tolist()
        post_processed_avatars = sensitive_unshuffled_avatars.iloc[map].reset_index(
            drop=True
        )

        return AvatarizationPipelineResult(
            privacy_metrics=privacy_job.result,
            signal_metrics=signal_job.result,
            post_processed_avatars=post_processed_avatars,
            avatarization_job_id=avatarization_job.id,
            signal_job_id=signal_job.id,
            privacy_job_id=privacy_job.id,
        )


def upload_batch_and_get_order(
    client: "ApiClient",
    training: pd.DataFrame,
    splits: List[pd.DataFrame],
    timeout: int = DEFAULT_TIMEOUT,
) -> Tuple[UUID, List[UUID], Dict[UUID, pd.Index]]:
    """Upload batches to the server
    Arguments
    ---------
        client:
            Api client
        training:
            Dataframe used to train the anonymization model. This dataframe should contain all modalities of the categorical variables.
        splits:
            All other batches
    Returns
    -------
        training_dataset_id:
            The dataset id of the training dataset
        datasets_split_ids:
            The dataset id of all other batches
        batch_mapping:
            The index mapping for each dataset batch
    """
    training_dataset = client.pandas_integration.upload_dataframe(
        training, timeout=timeout
    )

    datasets_split_ids = [
        client.pandas_integration.upload_dataframe(split, timeout=timeout).id
        for split in splits
    ]
    batch_mapping: Dict[UUID, pd.Index] = {training_dataset.id: training.index}
    for dataset, dataframe in zip(datasets_split_ids, splits):
        batch_mapping[dataset] = dataframe.index

    return training_dataset.id, datasets_split_ids, batch_mapping


def download_avatar_dataframe_from_batch(
    client: "ApiClient",
    avatarization_batch_result: AvatarizationBatchResult,
    timeout: int = DEFAULT_TIMEOUT,
    filetype: FileType = FileType.parquet,
) -> pd.DataFrame:
    """Download the shuffled avatar dataframe from batch result.
    Arguments
    ---------
        client:
            Api client
        avatarization_batch_result:
            Result of the batch avatarization
    Returns
    -------
        the concatenated shuffled avatar dataframe
    """
    training_df = client.pandas_integration.download_dataframe(
        str(avatarization_batch_result.training_result.avatars_dataset.id),
        filetype=filetype,
        timeout=timeout,
    )
    splits_df = [
        client.pandas_integration.download_dataframe(
            str(batch_results.avatars_dataset.id),
            filetype=filetype,
            timeout=timeout,
        )
        for batch_results in avatarization_batch_result.batch_results
    ]
    return pd.concat([training_df] + splits_df)


def download_sensitive_unshuffled_avatar_dataframe_from_batch(
    client: "ApiClient",
    avatarization_batch_result: AvatarizationBatchResult,
    order: Dict[UUID, pd.Index],
    timeout: int = DEFAULT_TIMEOUT,
    filetype: FileType = FileType.parquet,
) -> pd.DataFrame:
    """Download the sensitive unshuffled avatar dataframe from batch result.

    The avatar dataframe is ordered in the original dataframe order.
    Arguments
    ---------
        client:
            Api client
        avatarization_batch_result:
            Result of the batch avatarization
        order:
            index order for each dataset batch
    Returns
    -------
        concatenated:
            the concatenated avatar dataframe with the row order of the original dataframe
    """
    avatar_training_id = (
        avatarization_batch_result.training_result.sensitive_unshuffled_avatars_datasets.id
    )
    original_training_id = avatarization_batch_result.training_result.original_id
    training_df = client.pandas_integration.download_dataframe(
        str(avatar_training_id), filetype=filetype, timeout=timeout
    )
    training_df.index = order[original_training_id]

    split_dfs = []
    for batch_results in avatarization_batch_result.batch_results:
        avatar_dataset_id = batch_results.sensitive_unshuffled_avatars_datasets.id
        original_dataset_id = batch_results.original_id

        split = client.pandas_integration.download_dataframe(
            str(avatar_dataset_id), filetype=filetype, timeout=timeout
        )
        split.index = order[original_dataset_id]
        split_dfs.append(split)

    concatenated = pd.concat([training_df] + split_dfs).sort_index()
    return concatenated


# This file has been generated - DO NOT MODIFY
