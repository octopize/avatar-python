# This file has been generated - DO NOT MODIFY
# API Version : 0.5.24-4466bfb89f205cdba801d50aa8d95901746011d1


import io
import itertools
import logging
import os
import tempfile
import time
import warnings
from contextlib import ExitStack
from copy import copy
from io import BytesIO, StringIO
from typing import (
    IO,
    TYPE_CHECKING,
    Any,
    AnyStr,
    BinaryIO,
    Callable,
    Dict,
    List,
    Literal,
    NoReturn,
    Optional,
    Protocol,
    Sequence,
    TextIO,
    Tuple,
    TypeVar,
    Union,
    cast,
    overload,
)
from uuid import UUID

import numpy as np
import pandas as pd
import pyarrow
from pydantic import BaseModel

from avatars.api_autogenerated import Auth as _Auth
from avatars.api_autogenerated import Compatibility as _Compatibility
from avatars.api_autogenerated import Datasets as _Datasets
from avatars.api_autogenerated import Health as _Health
from avatars.api_autogenerated import Jobs as _Jobs
from avatars.api_autogenerated import Metrics as _Metrics
from avatars.api_autogenerated import Reports as _Reports
from avatars.api_autogenerated import Stats as _Stats
from avatars.api_autogenerated import Users as _Users
from avatars.models import (
    AvatarizationBatchJob,
    AvatarizationBatchJobCreate,
    AvatarizationBatchResult,
    AvatarizationJob,
    AvatarizationJobCreate,
    AvatarizationMultiTableJob,
    AvatarizationMultiTableJobCreate,
    AvatarizationPipelineCreate,
    AvatarizationPipelineResult,
    AvatarizationWithTimeSeriesJob,
    AvatarizationWithTimeSeriesJobCreate,
    ClusterStats,
    ColumnDetail,
    ColumnType,
    CompatibilityResponse,
    Contributions,
    CreateDataset,
    CreateUser,
    Dataset,
    ExplainedVariance,
    FileType,
    ForgottenPasswordRequest,
    GenericJob,
    JobStatus,
    Login,
    LoginResponse,
    PatchDataset,
    PrivacyMetrics,
    PrivacyMetricsBatchJob,
    PrivacyMetricsBatchJobCreate,
    PrivacyMetricsGeolocationJob,
    PrivacyMetricsGeolocationJobCreate,
    PrivacyMetricsJob,
    PrivacyMetricsJobCreate,
    PrivacyMetricsMultiTableJob,
    PrivacyMetricsMultiTableJobCreate,
    PrivacyMetricsParameters,
    PrivacyMetricsWithTimeSeriesJob,
    PrivacyMetricsWithTimeSeriesJobCreate,
    Processor,
    Projections,
    Report,
    ReportCreate,
    ReportFromBatchCreate,
    ReportFromDataCreate,
    ReportGeolocationPrivacyCreate,
    ResetPasswordRequest,
    SignalMetrics,
    SignalMetricsBatchJob,
    SignalMetricsBatchJobCreate,
    SignalMetricsJob,
    SignalMetricsJobCreate,
    SignalMetricsParameters,
    SignalMetricsWithTimeSeriesJob,
    SignalMetricsWithTimeSeriesJobCreate,
    User,
)

if TYPE_CHECKING:
    from avatars.client import ApiClient
    from avatars._typing import FileLikeInterface, HttpxFile

from avatars._typing import is_file_like

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())
DEFAULT_RETRY_TIMEOUT = 60
DEFAULT_TIMEOUT = 5


class Timeout(Exception):
    pass


T = TypeVar("T")


def to_common_type(s: str) -> ColumnType:
    if "float" in s:
        return ColumnType.float
    if "int" in s:
        return ColumnType.int
    if "bool" in s:
        return ColumnType.bool
    if "datetime" in s:
        return ColumnType.datetime
    if "object" in s or s == "category" or s == "str":
        return ColumnType.category
    raise TypeError(f"Unknown column type: '{s}'")


class Auth:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def login(
        self,
        request: Login,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> LoginResponse:
        """Login the user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).login(*args, **kwargs)

    def forgotten_password(
        self,
        request: ForgottenPasswordRequest,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).forgotten_password(*args, **kwargs)

    def reset_password(
        self,
        request: ResetPasswordRequest,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Auth(self.client).reset_password(*args, **kwargs)


class Compatibility:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def is_client_compatible(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> CompatibilityResponse:
        """Verify if the client is compatible with the API."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Compatibility(self.client).is_client_compatible(*args, **kwargs)


class Datasets:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def create_dataset_from_stream(
        self,
        request: Optional[
            Union["FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        name: Optional[str] = None,
        source: Optional[
            Union[str, "FileLikeInterface[bytes]"]
        ] = None,  # optional because we still have to support the old way # TODO: Remove once deprecated
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Create a dataset by streaming chunks of the dataset.

        DEPRECATED: Please use create_dataset instead.
        """

        warnings.warn(
            DeprecationWarning(
                "create_dataset_from_stream is deprecated. Use create_dataset instead."
            )
        )

        _source: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = (request or source)
        return self.create_dataset(source=_source, name=name, timeout=timeout)  # type: ignore[arg-type]

    def create_dataset(
        self,
        request: Optional[
            Union["FileLikeInterface[str]", "FileLikeInterface[bytes]"]
        ] = None,  # TODO: Remove once deprecated
        name: Optional[str] = None,
        source: Optional[
            Union[
                str,
                list[str],
                "FileLikeInterface[bytes]",
            ]
        ] = None,  # optional because we still have to support the old way
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Create a dataset from file upload."""

        if request:
            warnings.warn(
                DeprecationWarning("request is deprecated. Use file instead.")
            )

        if request is not None and source is not None:
            raise ValueError("You cannot pass both request and source.")

        if request is None and source is None:
            raise ValueError("You need to pass in a source.")

        _source = request or source

        if _source is None:
            raise ValueError("You need to pass in a source.")

        with ExitStack() as stack:
            file_arguments = self._create_httpx_file_argument(_source, stack)

            kwargs = {
                "method": "post",
                "url": f"/datasets/stream",
                "timeout": timeout,
                "file": file_arguments,
                "params": dict(
                    name=name,
                ),
            }

            result = self.client.request(**kwargs)  # type: ignore[arg-type]
        return Dataset(**result)

    def download_dataset_as_stream(
        self,
        id: str,
        destination: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> BytesIO:
        """Download a dataset by streaming chunks of it.

        DEPRECATED: Please use download_dataset instead.
        """
        warnings.warn(
            DeprecationWarning(
                "download_dataset_as_stream is deprecated. Use download_dataset instead."
            )
        )

        # Ignoring return type because download_dataset's logic makes sure
        # that the return type will be BytesIO
        # No point in going through the hassle of using typing.overload for something
        # that will be removed soon
        return self.download_dataset(  # type: ignore[return-value]
            id,
            destination=destination,
            timeout=timeout,
            from_download_as_stream=True,
        )

    def download_dataset(
        self,
        id: str,
        destination: Optional[
            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
        ] = None,
        filetype: Optional[FileType] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        from_download_as_stream: bool = False,  # TODO: Remove once deprecated
    ) -> Optional[Union[BytesIO, str]]:
        """Download a dataset."""

        if destination is None and not from_download_as_stream:
            # Old return value when using download_dataset
            filetype = filetype or FileType.csv

        kwargs = {
            "method": "get",
            "url": f"/datasets/{id}/download/stream",
            "timeout": timeout,
            "should_stream": True,
            "params": {
                "filetype": filetype,
            },
        }

        received_buffer = cast(BytesIO, self.client.request(**kwargs))  # type: ignore[arg-type]

        received_buffer.seek(0)

        if destination is None:
            warnings.warn(
                DeprecationWarning(
                    "Please specify the destination argument. The return type will change in the future."
                )
            )
            # TODO: Remove once deprecated
            if from_download_as_stream:
                # Old return value when using download_dataset_as_stream
                return received_buffer
            else:
                # Old return value when using download_dataset (with Filetype.csv as default)
                received = received_buffer.read()
                if filetype is FileType.csv:
                    # Old return value when using download_dataset (with Filetype.csv as default)
                    return received.decode()
                elif filetype is FileType.parquet:
                    raise ValueError(
                        "Can't download parquet files as string. Please use a different destination."
                    )

        if isinstance(destination, str):
            # User wants to save the file to disk
            with open(destination, "wb") as f:
                f.write(received_buffer.read())
            return None

        if is_file_like(destination):
            # User wants to save the file to a buffer he provided
            as_bytes = received_buffer.read()
            del received_buffer

            # We don't know the type of the buffer, so we try to write the bytes to it
            # and if it fails, we assume it's a text buffer and try to decode the bytes
            try:
                destination.write(as_bytes)  # type: ignore[arg-type]
            except TypeError as e:
                if "string argument expected" in str(e.args[0]):
                    destination.write(as_bytes.decode())
                else:
                    raise e
            destination.seek(0)

            return None

        raise TypeError(
            f"Expected destination to be a string or a buffer, got {type(destination)} instead"
        )

    def _create_httpx_file_argument(
        self,
        source: Union[
            str,
            list[str],
            "FileLikeInterface[bytes]",
            "FileLikeInterface[str]",  # TODO: remove that possibility once deprecated
        ],
        stack: ExitStack,
    ) -> List["HttpxFile"]:
        if source and is_file_like(source):
            # User provided file like object.
            # It is his responsibility to close it.

            # We try reading the file to check if it's binary or not
            # HTTPX handles binary files only

            single_char = source.read(1)

            if isinstance(single_char, bytes):
                # undo previous read operation
                # can only be done on binary file handles
                source.seek(-1, os.SEEK_CUR)

                # File is binary, we can upload it as is
                return [("file", source)]

            # TODO: When deprecating, raise an error instead of a warning
            warnings.warn(
                DeprecationWarning(
                    "You are trying to upload a text file. This is deprecated. "
                    "Please open the file in binary mode."
                )
            )

            # We create a new temporary file, write the content of the source file to it
            # and then upload the temporary file, but this time opened in binary mode (default)
            temporary_file = stack.enter_context(tempfile.NamedTemporaryFile())
            temporary_file.write(single_char.encode())
            temporary_file.write(source.read().encode())
            temporary_file.seek(0)

            return [("file", temporary_file)]

        if isinstance(source, str):
            source = [source]

        if isinstance(source, list):
            # List of files to upload as one dataset.
            # This is especially useful for large parquet files.
            return [
                ("file", stack.enter_context(open(file_path, "rb")))
                for file_path in source
            ]

        raise TypeError(
            f"Expected source to be a string or a buffer, got {type(source)} instead."
        )

    def find_all_datasets_by_user(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[Dataset]:
        """List all datasets of the current_user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Datasets(self.client).find_all_datasets_by_user(*args, **kwargs)

    def get_dataset(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Get a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).get_dataset(*args, **kwargs)

    def patch_dataset(
        self,
        request: PatchDataset,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Modify a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
            id,
        ]

        return _Datasets(self.client).patch_dataset(*args, **kwargs)

    def analyze_dataset(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Dataset:
        """Start the analysis of a dataset."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).analyze_dataset(*args, **kwargs)

    def get_dataset_correlations(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Get a dataset's correlations."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Datasets(self.client).get_dataset_correlations(*args, **kwargs)


class Health:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_root(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify server health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_root(*args, **kwargs)

    def get_health(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify server health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_health(*args, **kwargs)

    def get_health_db(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Verify connection to the db health."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Health(self.client).get_health_db(*args, **kwargs)


class Jobs:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def find_all_jobs_by_user(
        self,
        nb_days: Optional[int] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[GenericJob]:
        """Retrieve all jobs executed by the current user.

        Jobs are filtered by execution date, by default only the last 5 days are displayed,
        a parameter can be provided to go further back in time.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            nb_days,
        ]

        return _Jobs(self.client).find_all_jobs_by_user(*args, **kwargs)

    def create_full_avatarization_job(
        self,
        request: AvatarizationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Create an avatarization job, then calculate metrics."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_full_avatarization_job(*args, **kwargs)

    def cancel_job(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> GenericJob:
        """Cancel any kind of job.

        If the job hadn't been started yet, revoke it.
        If the job is ongoing, gently kill it.
        If the job is done, do nothing.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).cancel_job(*args, **kwargs)

    def create_avatarization_job(
        self,
        request: AvatarizationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Create an avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_job(*args, **kwargs)

    def create_avatarization_batch_job(
        self,
        request: AvatarizationBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationBatchJob:
        """Create an avatarization batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_batch_job(*args, **kwargs)

    def create_avatarization_with_time_series_job(
        self,
        request: AvatarizationWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationWithTimeSeriesJob:
        """Create an avatarization with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_with_time_series_job(
            *args, **kwargs
        )

    def create_avatarization_multi_table_job(
        self,
        request: AvatarizationMultiTableJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationMultiTableJob:
        """Create an avatarization for relational data."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_avatarization_multi_table_job(*args, **kwargs)

    def create_signal_metrics_job(
        self,
        request: SignalMetricsJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsJob:
        """Create a signal metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_job(*args, **kwargs)

    def create_privacy_metrics_job(
        self,
        request: PrivacyMetricsJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsJob:
        """Create a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_job(*args, **kwargs)

    def create_privacy_metrics_batch_job(
        self,
        request: PrivacyMetricsBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsBatchJob:
        """Create a privacy metrics batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_batch_job(*args, **kwargs)

    def create_privacy_metrics_time_series_job(
        self,
        request: PrivacyMetricsWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsWithTimeSeriesJob:
        """Create a privacy metrics with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_time_series_job(
            *args, **kwargs
        )

    def create_signal_metrics_time_series_job(
        self,
        request: SignalMetricsWithTimeSeriesJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsWithTimeSeriesJob:
        """Create a signal metrics with time series job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_time_series_job(*args, **kwargs)

    def create_signal_metrics_batch_job(
        self,
        request: SignalMetricsBatchJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsBatchJob:
        """Create a signal metrics batch job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_signal_metrics_batch_job(*args, **kwargs)

    def create_privacy_metrics_multi_table_job(
        self,
        request: PrivacyMetricsMultiTableJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsMultiTableJob:
        """Create a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_multi_table_job(
            *args, **kwargs
        )

    def create_privacy_metrics_geolocation_job(
        self,
        request: PrivacyMetricsGeolocationJobCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsGeolocationJob:
        """Create a geolocation privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Jobs(self.client).create_privacy_metrics_geolocation_job(
            *args, **kwargs
        )

    def get_privacy_metrics_geolocation_job(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsGeolocationJob:
        """Get a geolocation privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_geolocation_job(*args, **kwargs)

    def get_avatarization_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationJob:
        """Get an avatarization job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_job(*args, **kwargs)

    def get_avatarization_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationBatchJob:
        """Get an avatarization batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_batch_job(*args, **kwargs)

    def get_avatarization_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationWithTimeSeriesJob:
        """Get an avatarization time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_time_series_job(*args, **kwargs)

    def get_avatarization_multi_table_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationMultiTableJob:
        """Get a multi table avatarization job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_avatarization_multi_table_job(*args, **kwargs)

    def get_signal_metrics(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsJob:
        """Get a signal metrics job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics(*args, **kwargs)

    def get_signal_metrics_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsBatchJob:
        """Get a signal metrics batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics_batch_job(*args, **kwargs)

    def get_signal_metrics_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> SignalMetricsWithTimeSeriesJob:
        """Get a signal metrics time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_signal_metrics_time_series_job(*args, **kwargs)

    def get_privacy_metrics(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsJob:
        """Get a privacy metrics job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics(*args, **kwargs)

    def get_privacy_metrics_batch_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsBatchJob:
        """Get a privacy metrics batch job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_batch_job(*args, **kwargs)

    def get_privacy_metrics_time_series_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsWithTimeSeriesJob:
        """Get a privacy metrics time series job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_time_series_job(*args, **kwargs)

    def get_privacy_metrics_multi_table_job(
        self,
        id: str,
        *,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> PrivacyMetricsMultiTableJob:
        """Get a privacy metrics multi table job."""

        kwargs: Dict[str, Any] = {
            "per_request_timeout": per_request_timeout,
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Jobs(self.client).get_privacy_metrics_multi_table_job(*args, **kwargs)


class Metrics:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_job_projections(
        self,
        job_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Projections:
        """Get the projections of records and avatars in 3D.

        See https://saiph.readthedocs.io/en/latest/ for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
        ]

        return _Metrics(self.client).get_job_projections(*args, **kwargs)

    def get_variable_contributions(
        self,
        job_id: str,
        dataset_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Contributions:
        """Get the contributions of the dataset variables within the fitted space.

        See https://saiph.readthedocs.io/en/latest for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
            dataset_id,
        ]

        return _Metrics(self.client).get_variable_contributions(*args, **kwargs)

    def get_explained_variance(
        self,
        job_id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> ExplainedVariance:
        """Get the explained variance of records.

        See https://saiph.readthedocs.io/en/latest/ for more information.

        Arguments
        ---------
            job_id:
                avatarization or privacy job id used to fit the model
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            job_id,
        ]

        return _Metrics(self.client).get_explained_variance(*args, **kwargs)


class Reports:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def create_report(
        self,
        request: ReportCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report(*args, **kwargs)

    def download_report(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Any:
        """Download a report."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Reports(self.client).download_report(*args, **kwargs)

    def create_report_from_data(
        self,
        request: ReportFromDataCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report without avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report_from_data(*args, **kwargs)

    def create_report_from_batch(
        self,
        request: ReportFromBatchCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report from batch job identifiers.

        The report will be generated with the worst privacy_metrics and the mean signal_metrics.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_report_from_batch(*args, **kwargs)

    def create_geolocation_privacy_report(
        self,
        request: ReportGeolocationPrivacyCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> Report:
        """Create an anonymization report without avatarization job."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Reports(self.client).create_geolocation_privacy_report(*args, **kwargs)


class Stats:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def get_cluster_stats(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> ClusterStats:
        """Get insights into the cluster's usage."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Stats(self.client).get_cluster_stats(*args, **kwargs)


class Users:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def find_users(
        self,
        email: Optional[str] = None,
        username: Optional[str] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> List[User]:
        """Get users, optionally filtering them by username or email.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            email,
            username,
        ]

        return _Users(self.client).find_users(*args, **kwargs)

    def create_user(
        self,
        request: CreateUser,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Create a user.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            request,
        ]

        return _Users(self.client).create_user(*args, **kwargs)

    def get_me(
        self,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Get my own user."""

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = []

        return _Users(self.client).get_me(*args, **kwargs)

    def get_user(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> User:
        """Get a user by id.

        This endpoint is protected with rate limiting.
        """

        kwargs: Dict[str, Any] = {
            "timeout": timeout,
        }

        args: List[Any] = [
            id,
        ]

        return _Users(self.client).get_user(*args, **kwargs)


class PandasIntegration:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def upload_dataframe(
        self,
        request: "pd.DataFrame",
        name: Optional[str] = None,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        should_stream: bool = False,
        identifier_variables: List[str] = [],
    ) -> Dataset:
        for col in request.columns:
            if pd.api.types.infer_dtype(request[col], skipna=True) in (
                "mixed-integer",
                "mixed",
            ):
                raise ValueError(
                    f"Expected column '{col}' should have either str or numeric values."
                    " Consider harmonizing columns prior to upload."
                )

        df_types = request.dtypes
        buffer = BytesIO()
        request.to_parquet(buffer, index=False, engine="pyarrow")
        buffer.seek(0)
        del request

        if should_stream:
            dataset = self.client.datasets.create_dataset_from_stream(
                buffer, timeout=timeout, name=name
            )
        else:
            dataset = self.client.datasets.create_dataset(
                source=buffer, timeout=timeout, name=name
            )
            columns = []
            for index, dtype in zip(df_types.index, df_types):
                if index in identifier_variables:
                    column_detail = ColumnDetail(
                        type=ColumnType.identifier, label=index
                    )
                else:
                    column_detail = ColumnDetail(
                        type=to_common_type(str(dtype)), label=index
                    )
                columns.append(column_detail)

            dataset = self.client.datasets.patch_dataset(
                id=str(dataset.id),
                request=PatchDataset(columns=columns),
            )
        return dataset

    def download_dataframe(
        self,
        id: str,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        should_stream: bool = False,
    ) -> pd.DataFrame:
        dataset_info = self.client.datasets.get_dataset(id, timeout=timeout)
        dataset_io: BytesIO
        if should_stream:
            dataset_io = self.client.datasets.download_dataset_as_stream(
                id, timeout=timeout
            )
        else:
            dataset_io = io.BytesIO()
            self.client.datasets.download_dataset(
                id, destination=dataset_io, timeout=timeout
            )

        dataset_io.seek(0)

        # We apply datetime columns separately as 'datetime' is not a valid pandas dtype
        dtypes = {
            c.label: c.type.value
            for c in dataset_info.columns or {}
            if c.type is not ColumnType.identifier
        }
        datetime_columns = [
            label for label, type in dtypes.items() if type == ColumnType.datetime.value
        ]

        # Remove datetime columns
        for label in list(dtypes.keys()):
            if label in datetime_columns:
                dtypes.pop(label, None)
        try:
            # We do copy() because read_parquet consumes the buffer, even on failure.
            df = pd.read_parquet(copy(dataset_io), engine="pyarrow")
        except pyarrow.lib.ArrowInvalid as e:
            if (
                not "Either the file is corrupted or this is not a parquet file."
                in str(e.args)
            ):
                raise e
            df = pd.read_csv(dataset_io)

        for name, dtype in dtypes.items():
            df[name] = df[name].astype(dtype)

        df[datetime_columns] = df[datetime_columns].astype("datetime64[ns]")

        return df


class Pipelines:
    def __init__(self, client: "ApiClient") -> None:
        self.client = client

    def avatarization_pipeline_with_processors(
        self,
        request: AvatarizationPipelineCreate,
        *,
        timeout: Optional[int] = DEFAULT_TIMEOUT,
        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
    ) -> AvatarizationPipelineResult:
        """Create a Pipeline of avatarization with processors."""

        # Upload the dataset if needed
        # User can either specify the dataset he already uploaded or let this pipeline upload it
        df = request.df.copy()
        original_dataset_id = (
            request.avatarization_job_create.parameters.dataset_id
            or self.client.pandas_integration.upload_dataframe(df, timeout=timeout).id
        )

        # Pre process the dataframe and upload it
        processors = request.processors
        for p in processors:
            df = p.preprocess(df)
        dataset = self.client.pandas_integration.upload_dataframe(
            df, timeout=per_request_timeout
        )

        # Avatarize the uploaded dataframe
        request.avatarization_job_create.parameters.dataset_id = dataset.id
        avatarization_job = self.client.jobs.create_avatarization_job(
            request.avatarization_job_create
        )
        print(f"launching avatarization job with id={avatarization_job.id}")

        avatarization_job = self.client.jobs.get_avatarization_job(
            str(avatarization_job.id),
            timeout=timeout,
            per_request_timeout=per_request_timeout,
        )
        if avatarization_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the avatarization job: {avatarization_job.error_message}"
            )

        if (
            avatarization_job.status == JobStatus.pending
            or not avatarization_job.result
        ):
            raise Timeout(
                f"The avatarization job '{avatarization_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        # Download the dataframe, postprocess it and upload the new dataframe
        sensitive_unshuffled_avatars = (
            self.client.pandas_integration.download_dataframe(
                str(avatarization_job.result.sensitive_unshuffled_avatars_datasets.id),
                timeout=timeout,
            )
        )
        for p in reversed(processors):
            sensitive_unshuffled_avatars = p.postprocess(
                request.df, sensitive_unshuffled_avatars
            )
        unshuffled_dataset = self.client.pandas_integration.upload_dataframe(
            sensitive_unshuffled_avatars, timeout=timeout
        )

        # Calculate privacy metrics on the post processed dataset vs the original one
        privacy_job = self.client.jobs.create_privacy_metrics_job(
            PrivacyMetricsJobCreate(
                parameters=PrivacyMetricsParameters(
                    original_id=original_dataset_id,
                    unshuffled_avatars_id=unshuffled_dataset.id,
                )
            ),
            timeout=per_request_timeout,
        )
        print(f"launching privacy metrics job with id={privacy_job.id}")

        # Calculate signal metrics
        signal_job = self.client.jobs.create_signal_metrics_job(
            SignalMetricsJobCreate(
                parameters=SignalMetricsParameters(
                    original_id=original_dataset_id, avatars_id=unshuffled_dataset.id
                )
            ),
            timeout=per_request_timeout,
        )
        print(f"launching signal metrics job with id={signal_job.id}")

        # Get the job results
        signal_job = self.client.jobs.get_signal_metrics(
            str(signal_job.id), timeout=timeout, per_request_timeout=per_request_timeout
        )
        privacy_job = self.client.jobs.get_privacy_metrics(
            str(privacy_job.id),
            timeout=timeout,
            per_request_timeout=per_request_timeout,
        )
        if signal_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the signal metrics job: {signal_job.error_message}"
            )
        if privacy_job.status == JobStatus.failure:
            raise Exception(
                f"Got error during the privacy metrics job: {privacy_job.error_message}"
            )

        if signal_job.status == JobStatus.pending or not signal_job.result:
            raise Timeout(
                f"The signal metrics job '{signal_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        if privacy_job.status == JobStatus.pending or not privacy_job.result:
            raise Timeout(
                f"The privacy metrics job '{privacy_job.id}' timed out."
                ""
                """Try increasing the timeout with the `timeout` parameter."""
            )

        # Shuffle sensitive_unshuffled_avatars for security reasons
        random_gen = np.random.default_rng()
        map = random_gen.permutation(sensitive_unshuffled_avatars.index.values).tolist()
        post_processed_avatars = sensitive_unshuffled_avatars.iloc[map].reset_index(
            drop=True
        )

        return AvatarizationPipelineResult(
            privacy_metrics=privacy_job.result,
            signal_metrics=signal_job.result,
            post_processed_avatars=post_processed_avatars,
            avatarization_job_id=avatarization_job.id,
            signal_job_id=signal_job.id,
            privacy_job_id=privacy_job.id,
        )


def upload_batch_and_get_order(
    client: "ApiClient",
    training: pd.DataFrame,
    splits: List[pd.DataFrame],
    timeout: int = DEFAULT_TIMEOUT,
) -> Tuple[UUID, List[UUID], Dict[UUID, pd.Index]]:
    """Upload batches to the server
    Arguments
    ---------
        client:
            Api client
        training:
            Dataframe used to train the anonymization model. This dataframe should contain all modalities of the categorical variables.
        splits:
            All other batches
    Returns
    -------
        training_dataset_id:
            The dataset id of the training dataset
        datasets_split_ids:
            The dataset id of all other batches
        batch_mapping:
            The index mapping for each dataset batch
    """
    training_dataset = client.pandas_integration.upload_dataframe(
        training, timeout=timeout
    )

    datasets_split_ids = [
        client.pandas_integration.upload_dataframe(split, timeout=timeout).id
        for split in splits
    ]
    batch_mapping: Dict[UUID, pd.Index] = {training_dataset.id: training.index}
    for dataset, dataframe in zip(datasets_split_ids, splits):
        batch_mapping[dataset] = dataframe.index

    return training_dataset.id, datasets_split_ids, batch_mapping


def download_avatar_dataframe_from_batch(
    client: "ApiClient",
    avatarization_batch_result: AvatarizationBatchResult,
    timeout: int = DEFAULT_TIMEOUT,
) -> pd.DataFrame:
    """Download the shuffled avatar dataframe from batch result.
    Arguments
    ---------
        client:
            Api client
        avatarization_batch_result:
            Result of the batch avatarization
    Returns
    -------
        the concatenated shuffled avatar dataframe
    """
    training_df = client.pandas_integration.download_dataframe(
        str(avatarization_batch_result.training_result.avatars_dataset.id),
        timeout=timeout,
    )
    splits_df = [
        client.pandas_integration.download_dataframe(
            str(batch_results.avatars_dataset.id),
            timeout=timeout,
        )
        for batch_results in avatarization_batch_result.batch_results
    ]
    return pd.concat([training_df] + splits_df)


def download_sensitive_unshuffled_avatar_dataframe_from_batch(
    client: "ApiClient",
    avatarization_batch_result: AvatarizationBatchResult,
    order: Dict[UUID, pd.Index],
    timeout: int = DEFAULT_TIMEOUT,
) -> pd.DataFrame:
    """Download the sensitive unshuffled avatar dataframe from batch result.

    The avatar dataframe is ordered in the original dataframe order.
    Arguments
    ---------
        client:
            Api client
        avatarization_batch_result:
            Result of the batch avatarization
        order:
            index order for each dataset batch
    Returns
    -------
        concatenated:
            the concatenated avatar dataframe with the row order of the original dataframe
    """
    avatar_training_id = (
        avatarization_batch_result.training_result.sensitive_unshuffled_avatars_datasets.id
    )
    original_training_id = avatarization_batch_result.training_result.original_id
    training_df = client.pandas_integration.download_dataframe(
        str(avatar_training_id), timeout=timeout
    )
    training_df.index = order[original_training_id]

    split_dfs = []
    for batch_results in avatarization_batch_result.batch_results:
        avatar_dataset_id = batch_results.sensitive_unshuffled_avatars_datasets.id
        original_dataset_id = batch_results.original_id

        split = client.pandas_integration.download_dataframe(
            str(avatar_dataset_id), timeout=timeout
        )
        split.index = order[original_dataset_id]
        split_dfs.append(split)

    concatenated = pd.concat([training_df] + split_dfs).sort_index()
    return concatenated


# This file has been generated - DO NOT MODIFY
