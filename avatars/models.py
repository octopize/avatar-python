# This file has been generated - DO NOT MODIFY
# API Version : 0.5.9-73f99ea7aa3ea04e3692c303523f75c9660e433a


from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Protocol, Union, runtime_checkable
from uuid import UUID

import pandas as pd
from pydantic import BaseModel, Field

# generated by datamodel-codegen:
#   filename:  <stdin>
#   timestamp: 2023-06-21T09:12:21+00:00


class AnalysisStatus(Enum):
    """
    An enumeration.
    """

    started = "started"
    done = "done"


class ClusterStats(BaseModel):
    n_active_tasks: Optional[int] = Field(None, title="N Active Tasks")
    available_concurrency: Optional[int] = Field(None, title="Available Concurrency")
    utilization_rate_100: Optional[int] = Field(None, title="Utilization Rate 100")
    status_message: Optional[str] = Field("", title="Status Message")


class ColumnStats(BaseModel):
    label: str = Field(..., title="Label")
    dtype: str = Field(..., title="Dtype")
    missing_number: int = Field(..., title="Missing Number")
    missing_proportion: float = Field(..., title="Missing Proportion")
    unique_number: int = Field(..., title="Unique Number")
    unique_proportion: float = Field(..., title="Unique Proportion")
    mean: Optional[float] = Field(None, title="Mean")
    median: Optional[float] = Field(None, title="Median")
    standard_deviation: Optional[float] = Field(None, title="Standard Deviation")
    mode: Optional[Union[str, int, float]] = Field(None, title="Mode")
    is_skewed: Optional[bool] = Field(None, title="Is Skewed")
    is_rare: Optional[bool] = Field(None, title="Is Rare")


class ColumnType(Enum):
    """
    An enumeration.
    """

    int = "int"
    bool = "bool"
    category = "category"
    float = "float"
    datetime = "datetime"


class CompatibilityResponse(BaseModel):
    message: str = Field(..., title="Message")
    most_recent_compatible_client: Optional[str] = Field(
        None, title="Most Recent Compatible Client"
    )


class Contributions(BaseModel):
    data: Dict[str, Dict[str, float]] = Field(..., title="Data")


class DatasetSummary(BaseModel):
    stats: List[ColumnStats] = Field(..., title="Stats")


class ExcludeCategoricalMethod(Enum):
    """
    The method to exclude categorical column.\n\nThere are several possible choices:\n\n- ``row_order``: **SENSITIVE** The excluded column will be linked to the original row order.\nThis is a violation of privacy.\n\n- ``coordinate_similarity``: The excluded column will be linked by individual similarity.
    """

    row_order = "row_order"
    coordinate_similarity = "coordinate_similarity"


class ExplainedVariance(BaseModel):
    raw: List[float] = Field(..., title="Raw")
    ratio: List[float] = Field(..., title="Ratio")


class ForgottenPasswordRequest(BaseModel):
    email: str = Field(..., title="Email")


class GenericParameters(BaseModel):
    pass


class GenericResult(BaseModel):
    pass


class ImputeMethod(Enum):
    """
    An enumeration.
    """

    knn = "knn"
    mode = "mode"
    median = "median"
    mean = "mean"
    fast_knn = "fast_knn"


class JobKind(Enum):
    """
    An enumeration.
    """

    avatarization = "avatarization"
    privacy_metrics = "privacy_metrics"
    signal_metrics = "signal_metrics"
    avatarization_batch = "avatarization_batch"
    privacy_metrics_batch = "privacy_metrics_batch"
    signal_metrics_batch = "signal_metrics_batch"


class JobProgress(BaseModel):
    completion_rate_100: int = Field(..., title="Completion Rate 100")
    name: Optional[str] = Field(None, title="Name")
    created_at: datetime = Field(..., title="Created At")


class JobStatus(Enum):
    """
    An enumeration.
    """

    pending = "pending"
    started = "started"
    success = "success"
    failure = "failure"
    killed = "killed"
    unknown = "unknown"


class LoginResponse(BaseModel):
    access_token: str = Field(..., title="Access Token")
    token_type: str = Field(..., title="Token Type")


class PrivacyBatchDatasetMapping(BaseModel):
    original_id: UUID = Field(..., title="Original Id")
    unshuffled_avatars_id: UUID = Field(..., title="Unshuffled Avatars Id")


class PrivacyMetricsTargets(BaseModel):
    hidden_rate: str = Field(..., title="Hidden Rate")
    local_cloaking: str = Field(..., title="Local Cloaking")
    distance_to_closest: str = Field(..., title="Distance To Closest")
    closest_distances_ratio: str = Field(..., title="Closest Distances Ratio")
    column_direct_match_protection: str = Field(
        ..., title="Column Direct Match Protection"
    )
    categorical_hidden_rate: str = Field(..., title="Categorical Hidden Rate")
    row_direct_match_protection: str = Field(..., title="Row Direct Match Protection")
    correlation_protection_rate: str = Field(..., title="Correlation Protection Rate")
    inference_continuous: str = Field(..., title="Inference Continuous")
    inference_categorical: str = Field(..., title="Inference Categorical")
    closest_rate: str = Field(..., title="Closest Rate")


class Projections(BaseModel):
    records: List[List[float]] = Field(..., title="Records")
    avatars: List[List[float]] = Field(..., title="Avatars")


class RareCategoricalMethod(Enum):
    """
    The method to replace rare modalities.\n\nAvailable rare modality replacement strategies are:\n\n- ``most_similar``: Rare modalities will be replaced with the modality of the most similar\n    records.\n\n- ``probabilistic``: Probabilities will be defined for each potential replacement value and\n    the rare modalities will be replaced following these probabilities in a\n    non-deterministic way.\n\n- ``missing``: Rare modalities will be replace with missing values.
    """

    most_similar = "most_similar"
    probabilistic = "probabilistic"
    missing = "missing"


class Report(BaseModel):
    id: UUID = Field(..., title="Id")
    user_id: UUID = Field(..., title="User Id")
    job_id: UUID = Field(..., title="Job Id")
    created_at: Optional[datetime] = Field(None, title="Created At")
    download_url: str = Field(..., title="Download Url")


class ReportCreate(BaseModel):
    avatarization_job_id: UUID = Field(..., title="Avatarization Job Id")
    privacy_job_id: UUID = Field(..., title="Privacy Job Id")
    signal_job_id: UUID = Field(..., title="Signal Job Id")


class ReportFromBatchCreate(BaseModel):
    avatarization_batch_job_id: UUID = Field(..., title="Avatarization Batch Job Id")
    privacy_batch_job_id: UUID = Field(..., title="Privacy Batch Job Id")
    signal_batch_job_id: UUID = Field(..., title="Signal Batch Job Id")


class ReportFromDataCreate(BaseModel):
    dataset_id: UUID = Field(..., title="Dataset Id")
    avatars_dataset_id: UUID = Field(..., title="Avatars Dataset Id")
    privacy_job_id: UUID = Field(..., title="Privacy Job Id")
    signal_job_id: UUID = Field(..., title="Signal Job Id")


class ResetPasswordRequest(BaseModel):
    email: str = Field(..., title="Email")
    new_password: str = Field(..., max_length=128, min_length=12, title="New Password")
    new_password_repeated: str = Field(
        ..., max_length=128, min_length=12, title="New Password Repeated"
    )
    token: UUID = Field(..., title="Token")


class SignalBatchDatasetMapping(BaseModel):
    original_id: UUID = Field(..., title="Original Id")
    avatars_id: UUID = Field(..., title="Avatars Id")


class SignalMetricsBaseParameters(BaseModel):
    seed: Optional[int] = Field(None, title="Seed")


class SignalMetricsBatchParameters(BaseModel):
    """
        Parameters to configure a SignalMetricsBatchJob.

    There are two main use-cases:
        1. Launching a SignalMetricsBatchJob after an AvatarizationBatchJob
        2. Launching a SignalMetricsBatchJob without applying the avatar method

    1. Launching a SignalMetricsBatchJob after an AvatarizationBatchJob

    By specifying ``avatarization_batch_job_id``, we can gather all the necessary information
    from the database, namely which batches to pair together when computing the metrics.

    You can add parameters in ``common_parameters`` that will apply to the training
    batch as well as all the other batches.

    If you want to manually select which avatar batch get's paired with which original batch,
    you'll have to refer to use-case 2.

    >>> parameters = SignalMetricsBatchParameters(
    ...     avatarization_batch_job_id='c43c7cc7-9b66-4293-b88f-b5dbd07e1f95',
    ... )
    >>> parameters = SignalMetricsBatchParameters(
    ...     avatarization_batch_job_id='c43c7cc7-9b66-4293-b88f-b5dbd07e1f95',
    ...     common_parameters=SignalMetricsBaseParameters(seed=42),
    ... )

    2. Launching a SignalMetricsBatchJob without applying the avatar method

    If you want more control over which avatar batch get's paired with which original batch,
    or you want to compute metrics on datasets that did not have the avatar method applied
    to them, you can follow the steps laid out below.

    You'll have to split and upload the original and avatar data batches
    before launching the SignalMetricsBatchJob.

    Suppose you have the original training batch dataset as ``training_original_id``,
    the avatar training batch dataset as ``training_avatars_id``,
    as well as a list of all the other batch dataset identifiers as
    ``original_dataset_ids``, and ``avatars_dataset_ids``, and that all these have been uploaded
    to the server.

    You need to specify the training dataset identifiers that will be used to
    fit the anonymization. This can be done with ``training_dataset_mapping``.
    With ``batch_dataset_mappings`` you specify which avatar batch get's paired
    with which original batch. Both of these use instances of ``SignalBatchDatasetMapping``.

    ``common_parameters`` is where you'll add any parameters that you want the
    computation to use with an instance of ``SignalMetricsBaseParameters``.

    Because you haven't launched an ``AvatarizationBatchJob``, you set the
    ``avatarization_batch_job_id`` identifier to ``None``.

    >>> training_original_id = 'cf8e8dae-5d52-43b6-a6e6-be246ee35185'
    >>> training_avatars_id = 'e9dcdd58-24ea-4028-84df-778c0c1b5777'
    >>> original_dataset_ids = [
    ...     '00d332f6-4971-4589-8d12-0d14d2d929ee',
    ...     '65ac8062-d232-4c85-8beb-36c8235826ee',
    ... ]
    >>> avatars_dataset_ids = [
    ...     '2fd3d8cc-157b-4bc4-9a92-bc54f1a57cb9',
    ...     '97c1d3a0-0f63-424a-92ba-147c76a13c66',
    ... ]
    >>> batch_dataset_mappings = [
    ...     SignalBatchDatasetMapping(
    ...         avatars_id=avatars_dataset_id, original_id=original_dataset_id
    ...     )
    ...     for original_dataset_id, avatars_dataset_id in zip(
    ...         original_dataset_ids, avatars_dataset_ids
    ...     )
    ... ]
    >>> parameters = SignalMetricsBatchParameters(
    ...     avatarization_batch_job_id=None,
    ...     common_parameters=SignalMetricsBaseParameters(seed=42),
    ...     training_dataset_mapping=SignalBatchDatasetMapping(
    ...         avatars_id=training_avatars_id,
    ...         original_id=training_original_id,
    ...     ),
    ...     batch_dataset_mappings=batch_dataset_mappings
    ... )
    """

    avatarization_batch_job_id: Optional[UUID] = Field(
        None,
        description="Identifier of the avatarization batch job that was launched prior to this signal metrics batch job. This has to be set to None if you are computing signal metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.",
        title="Avatarization Batch Job Id",
    )
    training_dataset_mapping: Optional[SignalBatchDatasetMapping] = Field(
        None,
        description="Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization.",
        title="Training Dataset Mapping",
    )
    common_parameters: Optional[SignalMetricsBaseParameters] = Field(
        None,
        description="Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch.",
        title="Common Parameters",
    )
    batch_dataset_mappings: Optional[List[SignalBatchDatasetMapping]] = Field(
        None,
        description="List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.",
        title="Batch Dataset Mappings",
    )


class SignalMetricsParameters(BaseModel):
    original_id: UUID = Field(..., title="Original Id")
    persistance_job_id: Optional[UUID] = Field(None, title="Persistance Job Id")
    avatarization_job_id: Optional[UUID] = Field(None, title="Avatarization Job Id")
    seed: Optional[int] = Field(None, title="Seed")
    avatars_id: UUID = Field(..., title="Avatars Id")


class SignalMetricsTargets(BaseModel):
    hellinger_mean: str = Field(..., title="Hellinger Mean")
    correlation_difference_ratio: str = Field(..., title="Correlation Difference Ratio")


class UserRole(Enum):
    """
    An enumeration.
    """

    admin = "admin"
    user = "user"


class ValidationError(BaseModel):
    loc: List[Union[str, int]] = Field(..., title="Location")
    msg: str = Field(..., title="Message")
    type: str = Field(..., title="Error Type")


class Login(BaseModel):
    grant_type: Optional[str] = Field(None, regex="password", title="Grant Type")
    username: str = Field(..., title="Username")
    password: str = Field(..., title="Password")
    scope: Optional[str] = Field("", title="Scope")
    client_id: Optional[str] = Field(None, title="Client Id")
    client_secret: Optional[str] = Field(None, title="Client Secret")


class CreateDataset(BaseModel):
    file: bytes = Field(..., title="File")


class ColumnDetail(BaseModel):
    type: ColumnType
    label: str = Field(..., title="Label")


class CreateUser(BaseModel):
    """
    Create a user, either with an email, or a username.\n\nThe choice will depend on how your server is setup.
    """

    username: Optional[str] = Field(None, title="Username")
    email: Optional[str] = Field(None, title="Email")
    role: Optional[UserRole] = UserRole.user
    password: Optional[str] = Field(
        None, max_length=128, min_length=12, title="Password"
    )


class Dataset(BaseModel):
    id: UUID = Field(..., title="Id")
    hash: str = Field(..., title="Hash")
    name: Optional[str] = Field(None, title="Name")
    columns: Optional[List[ColumnDetail]] = Field(None, title="Columns")
    download_url: str = Field(..., title="Download Url")
    analysis_status: Optional[AnalysisStatus] = None
    analysis_duration: Optional[float] = Field(None, title="Analysis Duration")
    nb_lines: Optional[int] = Field(None, title="Nb Lines")
    nb_dimensions: int = Field(..., title="Nb Dimensions")
    summary: Optional[DatasetSummary] = None


class ExcludeCategoricalParameters(BaseModel):
    """
    Parameters to exclude some variables from the anonymization before re-assigning.\n\nThe use of this parameter is recommended when the data contains categorical variables\nwith a large number of modalities.
    """

    exclude_cardinality_threshold: int = Field(
        ...,
        description="Threshold defining the minimum cardinality of a variable to be excluded. Any categorical variable with a number of modalities greater or equal to this threshold will be excluded from the anonymization and re-assigned probabilistically.",
        ge=1.0,
        title="Exclude Cardinality Threshold",
    )
    exclude_replacement_strategy: ExcludeCategoricalMethod = Field(
        ..., description="See ``ExcludeCategoricalMethod``."
    )
    rare_occurence_threshold: int = Field(
        ...,
        description="Maximum number of occurrences for a modality in an excluded variable to be considered as rare. Modalities considered as rare will be removed and replaced by other modalities prior to being re-assigned. This prevents the risk of re-identification based on rare modalities.",
        ge=0.0,
        title="Rare Occurence Threshold",
    )
    rare_replacement_strategy: Optional[RareCategoricalMethod] = Field(
        None, description="See ``RareCategoricalMethod``."
    )
    number_reference_records: Optional[int] = Field(
        None,
        description="Number of training records to use to compute inter-records distances. Set to ``None`` to use all records, beware this may lead to long computational time. If ``number_reference_records`` is too low, it will be corrected to ensure each value combination to be replaced is represented in the sample. If ``number_reference_records`` is larger than the number of records, it will be corrected to the number of records.",
        title="Number Reference Records",
    )


class GenericJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[GenericResult] = None
    parameters: GenericParameters
    current_progress: Optional[JobProgress] = None


class HTTPValidationError(BaseModel):
    detail: Optional[List[ValidationError]] = Field(None, title="Detail")


class ImputationParameters(BaseModel):
    method: Optional[ImputeMethod] = None
    k: Optional[int] = Field(None, title="K")
    training_fraction: Optional[float] = Field(None, title="Training Fraction")


class PrivacyMetrics(BaseModel):
    hidden_rate: float = Field(..., title="Hidden Rate")
    local_cloaking: float = Field(..., title="Local Cloaking")
    distance_to_closest: float = Field(..., title="Distance To Closest")
    closest_distances_ratio: float = Field(..., title="Closest Distances Ratio")
    column_direct_match_protection: float = Field(
        ..., title="Column Direct Match Protection"
    )
    categorical_hidden_rate: float = Field(..., title="Categorical Hidden Rate")
    row_direct_match_protection: float = Field(..., title="Row Direct Match Protection")
    correlation_protection_rate: Optional[float] = Field(
        None, title="Correlation Protection Rate"
    )
    inference_continuous: Optional[float] = Field(None, title="Inference Continuous")
    inference_categorical: Optional[float] = Field(None, title="Inference Categorical")
    closest_rate: Optional[float] = Field(None, title="Closest Rate")
    targets: PrivacyMetricsTargets = Field(..., title="Targets")


class PrivacyMetricsBaseParameters(BaseModel):
    imputation: Optional[ImputationParameters] = None
    ncp: Optional[int] = Field(None, title="Ncp")
    seed: Optional[int] = Field(None, title="Seed")
    use_categorical_reduction: Optional[bool] = Field(
        None, title="Use Categorical Reduction"
    )
    known_variables: Optional[List[str]] = Field(None, title="Known Variables")
    target: Optional[str] = Field(None, title="Target")
    closest_rate_percentage_threshold: Optional[float] = Field(
        None, title="Closest Rate Percentage Threshold"
    )
    closest_rate_ratio_threshold: Optional[float] = Field(
        None, title="Closest Rate Ratio Threshold"
    )


class PrivacyMetricsBatchParameters(BaseModel):
    """
        Parameters to configure a PrivacyMetricsBatchJob.

    There are two main use-cases:
        1. Launching a PrivacyMetricsBatchJob after an AvatarizationBatchJob
        2. Launching a PrivacyMetricsBatchJob without applying the avatar method

    1. Launching a PrivacyMetricsBatchJob after an AvatarizationBatchJob

    By specifying ``avatarization_batch_job_id``, we can gather all the necessary information
    from the database, namely which batches to pair together when computing the metrics.

    You can add parameters in ``training_dataset_mapping`` that will apply to the training
    batch as well as all the other batches.

    If you want to manually select which avatar batch get's paired with which original batch,
    you'll have to refer to use-case 2.

    >>> parameters = PrivacyMetricsBatchParameters(
    ...     avatarization_batch_job_id='c43c7cc7-9b66-4293-b88f-b5dbd07e1f95',
    ... )
    >>> parameters = PrivacyMetricsBatchParameters(
    ...     avatarization_batch_job_id='c43c7cc7-9b66-4293-b88f-b5dbd07e1f95',
    ...     common_parameters=PrivacyMetricsBaseParameters(ncp=10),
    ... )

    2. Launching a PrivacyMetricsBatchJob without applying the avatar method

    If you want more control over which avatar batch get's paired with which original batch,
    or you want to compute metrics on datasets that did not have the avatar method applied
    to them, you can follow the steps laid out below.

    You'll have to split and upload the original and unshuffled avatar data batches
    before launching the PrivacyMetricsBatchJob.

    Suppose you have the original training batch dataset as ``training_original_id``,
    the unshuffled avatars training batch dataset as ``training_avatars_id``,
    as well as a list of all the other batch dataset identifiers as
    ``original_dataset_ids``, and ``avatars_dataset_ids``, and that all these have been uploaded
    to the server.

    You need to specify the training dataset identifiers that will be used to
    fit the anonymization.
    This can be done with ``training_dataset_mapping``.
    With ``batch_dataset_mappings`` you specify which avatar batch get's paired
    with which original batch. Both of these use instances of ``PrivacyBatchDatasetMapping``.

    ``common_parameters`` is where you'll add any parameters
    that you want the computation to use with an instance of ``PrivacyMetricsBaseParameters``.

    Because you haven't launched an ``AvatarizationBatchJob``, you have to set the
    ``avatarization_batch_job_id`` identifier to ``None``.

    >>> training_original_id = 'cf8e8dae-5d52-43b6-a6e6-be246ee35185'
    >>> training_avatars_id = 'e9dcdd58-24ea-4028-84df-778c0c1b5777'
    >>> original_dataset_ids = [
    ...     '00d332f6-4971-4589-8d12-0d14d2d929ee',
    ...     '65ac8062-d232-4c85-8beb-36c8235826ee',
    ... ]
    >>> avatars_dataset_ids = [
    ...     '2fd3d8cc-157b-4bc4-9a92-bc54f1a57cb9',
    ...     '97c1d3a0-0f63-424a-92ba-147c76a13c66',
    ... ]
    >>> batch_dataset_mappings = [
    ...     PrivacyBatchDatasetMapping(
    ...         unshuffled_avatars_id=avatars_dataset_id, original_id=original_dataset_id
    ...     )
    ...     for original_dataset_id, avatars_dataset_id in zip(
    ...         original_dataset_ids, avatars_dataset_ids
    ...     )
    ... ]
    >>> parameters = PrivacyMetricsBatchParameters(
    ...     avatarization_batch_job_id=None,
    ...     common_parameters=PrivacyMetricsBaseParameters(
    ...         ncp=10,
    ...     ),
    ...        training_dataset_mapping = PrivacyBatchDatasetMapping(
    ...         unshuffled_avatars_id=training_avatars_id,
    ...         original_id=training_original_id
    ...        ),
    ...     batch_dataset_mappings=batch_dataset_mappings
    ... )
    """

    avatarization_batch_job_id: Optional[UUID] = Field(
        None,
        description="Identifier of the avatarization batch job that was launched prior to this privacy metrics batch job. This has to be set to None if you are computing privacy metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.",
        title="Avatarization Batch Job Id",
    )
    training_dataset_mapping: Optional[PrivacyBatchDatasetMapping] = Field(
        None,
        description="Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization.",
        title="Training Dataset Mapping",
    )
    common_parameters: Optional[PrivacyMetricsBaseParameters] = Field(
        None,
        description="Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch.",
        title="Common Parameters",
    )
    batch_dataset_mappings: Optional[List[PrivacyBatchDatasetMapping]] = Field(
        None,
        description="List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.",
        title="Batch Dataset Mappings",
    )


class PrivacyMetricsParameters(BaseModel):
    original_id: UUID = Field(..., title="Original Id")
    persistance_job_id: Optional[UUID] = Field(None, title="Persistance Job Id")
    avatarization_job_id: Optional[UUID] = Field(None, title="Avatarization Job Id")
    imputation: Optional[ImputationParameters] = None
    ncp: Optional[int] = Field(None, title="Ncp")
    seed: Optional[int] = Field(None, title="Seed")
    use_categorical_reduction: Optional[bool] = Field(
        None, title="Use Categorical Reduction"
    )
    known_variables: Optional[List[str]] = Field(None, title="Known Variables")
    target: Optional[str] = Field(None, title="Target")
    closest_rate_percentage_threshold: Optional[float] = Field(
        None, title="Closest Rate Percentage Threshold"
    )
    closest_rate_ratio_threshold: Optional[float] = Field(
        None, title="Closest Rate Ratio Threshold"
    )
    unshuffled_avatars_id: UUID = Field(..., title="Unshuffled Avatars Id")


class PrivacyMetricsPerBatchResult(BaseModel):
    hidden_rate: float = Field(..., title="Hidden Rate")
    local_cloaking: float = Field(..., title="Local Cloaking")
    distance_to_closest: float = Field(..., title="Distance To Closest")
    closest_distances_ratio: float = Field(..., title="Closest Distances Ratio")
    column_direct_match_protection: float = Field(
        ..., title="Column Direct Match Protection"
    )
    categorical_hidden_rate: float = Field(..., title="Categorical Hidden Rate")
    row_direct_match_protection: float = Field(..., title="Row Direct Match Protection")
    correlation_protection_rate: Optional[float] = Field(
        None, title="Correlation Protection Rate"
    )
    inference_continuous: Optional[float] = Field(None, title="Inference Continuous")
    inference_categorical: Optional[float] = Field(None, title="Inference Categorical")
    closest_rate: Optional[float] = Field(None, title="Closest Rate")
    targets: PrivacyMetricsTargets = Field(..., title="Targets")
    original_id: UUID = Field(..., title="Original Id")
    unshuffled_avatars_id: UUID = Field(..., title="Unshuffled Avatars Id")


class SignalMetrics(BaseModel):
    hellinger_mean: float = Field(..., title="Hellinger Mean")
    hellinger_std: float = Field(..., title="Hellinger Std")
    correlation_difference_ratio: Optional[float] = Field(
        None, title="Correlation Difference Ratio"
    )
    targets: SignalMetricsTargets = Field(..., title="Targets")


class SignalMetricsBatchJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.signal_metrics_batch
    parameters: SignalMetricsBatchParameters


class SignalMetricsJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[SignalMetrics] = None
    parameters: SignalMetricsParameters
    current_progress: Optional[JobProgress] = None


class SignalMetricsJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.signal_metrics
    parameters: SignalMetricsParameters


class SignalMetricsPerBatchResult(BaseModel):
    hellinger_mean: float = Field(..., title="Hellinger Mean")
    hellinger_std: float = Field(..., title="Hellinger Std")
    correlation_difference_ratio: Optional[float] = Field(
        None, title="Correlation Difference Ratio"
    )
    targets: SignalMetricsTargets = Field(..., title="Targets")
    original_id: UUID = Field(..., title="Original Id")
    avatars_id: UUID = Field(..., title="Avatars Id")


class User(BaseModel):
    id: UUID = Field(..., title="Id")
    organization_id: UUID = Field(..., title="Organization Id")
    username: Optional[str] = Field(None, title="Username")
    email: Optional[str] = Field(None, title="Email")
    role: Optional[UserRole] = UserRole.user
    max_allowed_dimensions_per_dataset: Optional[int] = Field(
        None, title="Max Allowed Dimensions Per Dataset"
    )
    max_allowed_lines_per_dataset: Optional[int] = Field(
        None, title="Max Allowed Lines Per Dataset"
    )


class PatchDataset(BaseModel):
    columns: List[ColumnDetail] = Field(..., title="Columns")


class AvatarizationBatchParameters(BaseModel):
    k: int = Field(..., title="K")
    column_weights: Optional[Dict[str, float]] = Field(None, title="Column Weights")
    ncp: Optional[int] = Field(None, title="Ncp")
    seed: Optional[int] = Field(None, title="Seed")
    imputation: Optional[ImputationParameters] = None
    use_categorical_reduction: Optional[bool] = Field(
        None, title="Use Categorical Reduction"
    )
    to_categorical_threshold: Optional[int] = Field(
        None, title="To Categorical Threshold"
    )
    exclude_categorical: Optional[ExcludeCategoricalParameters] = None
    training_dataset_id: UUID = Field(..., title="Training Dataset Id")
    dataset_ids: List[UUID] = Field(..., min_items=1, title="Dataset Ids")


class AvatarizationParameters(BaseModel):
    k: int = Field(..., title="K")
    column_weights: Optional[Dict[str, float]] = Field(None, title="Column Weights")
    ncp: Optional[int] = Field(None, title="Ncp")
    seed: Optional[int] = Field(None, title="Seed")
    imputation: Optional[ImputationParameters] = None
    use_categorical_reduction: Optional[bool] = Field(
        None, title="Use Categorical Reduction"
    )
    to_categorical_threshold: Optional[int] = Field(
        None, title="To Categorical Threshold"
    )
    exclude_categorical: Optional[ExcludeCategoricalParameters] = None
    dataset_id: UUID = Field(..., title="Dataset Id")


class AvatarizationPerBatchResult(BaseModel):
    privacy_metrics: Optional[PrivacyMetrics] = None
    signal_metrics: Optional[SignalMetrics] = None
    avatars_dataset: Dataset
    sensitive_unshuffled_avatars_datasets: Dataset
    original_id: UUID = Field(..., title="Original Id")


class AvatarizationResult(BaseModel):
    privacy_metrics: Optional[PrivacyMetrics] = None
    signal_metrics: Optional[SignalMetrics] = None
    avatars_dataset: Dataset
    sensitive_unshuffled_avatars_datasets: Dataset


class PrivacyMetricsBatchJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.privacy_metrics_batch
    parameters: PrivacyMetricsBatchParameters


class PrivacyMetricsBatchResult(BaseModel):
    worst_metrics: PrivacyMetrics
    mean_metrics: PrivacyMetrics
    training_metrics: PrivacyMetricsPerBatchResult
    batch_metrics: List[PrivacyMetricsPerBatchResult] = Field(
        ..., title="Batch Metrics"
    )


class PrivacyMetricsJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[PrivacyMetrics] = None
    parameters: PrivacyMetricsParameters
    current_progress: Optional[JobProgress] = None


class PrivacyMetricsJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.privacy_metrics
    parameters: PrivacyMetricsParameters


class SignalMetricsBatchResult(BaseModel):
    mean_metrics: SignalMetrics
    training_metrics: SignalMetricsPerBatchResult
    batch_metrics: List[SignalMetricsPerBatchResult] = Field(..., title="Batch Metrics")


class AvatarizationBatchJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.avatarization_batch
    parameters: AvatarizationBatchParameters


class AvatarizationBatchResult(BaseModel):
    privacy_metrics: Optional[PrivacyMetrics] = None
    signal_metrics: Optional[SignalMetrics] = None
    training_result: AvatarizationPerBatchResult
    batch_results: List[AvatarizationPerBatchResult] = Field(..., title="Batch Results")


class AvatarizationJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[AvatarizationResult] = None
    parameters: AvatarizationParameters
    current_progress: Optional[JobProgress] = None


class AvatarizationJobCreate(BaseModel):
    kind: Optional[JobKind] = JobKind.avatarization
    parameters: AvatarizationParameters


class PrivacyMetricsBatchJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[PrivacyMetricsBatchResult] = None
    parameters: PrivacyMetricsBatchParameters
    current_progress: Optional[JobProgress] = None


class SignalMetricsBatchJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[SignalMetricsBatchResult] = None
    parameters: SignalMetricsBatchParameters
    current_progress: Optional[JobProgress] = None


class AvatarizationBatchJob(BaseModel):
    id: UUID = Field(..., title="Id")
    status: JobStatus
    error_message: Optional[str] = Field(None, title="Error Message")
    traceback: Optional[str] = Field(None, title="Traceback")
    result: Optional[AvatarizationBatchResult] = None
    parameters: AvatarizationBatchParameters
    current_progress: Optional[JobProgress] = None


@runtime_checkable
class Processor(Protocol):
    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:
        ...

    def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:
        ...


class AvatarizationPipelineCreate(BaseModel):
    class Config:
        arbitrary_types_allowed = True

    avatarization_job_create: AvatarizationJobCreate
    processors: List[Processor] = []
    df: pd.DataFrame


class AvatarizationPipelineResult(BaseModel):
    class Config:
        arbitrary_types_allowed = True

    privacy_metrics: PrivacyMetrics
    signal_metrics: SignalMetrics
    post_processed_avatars: pd.DataFrame
    avatarization_job_id: UUID
    signal_job_id: UUID
    privacy_job_id: UUID
