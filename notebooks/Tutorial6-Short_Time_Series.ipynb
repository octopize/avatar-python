{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a24fbbf",
   "metadata": {},
   "source": [
    "# Tutorial 6: Short Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36f0ea",
   "metadata": {},
   "source": [
    "In this tutorial, we will perform the avatarization of data that contains time series. The approach presented here can be used to anonymize time series that contain a small number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ab3ce",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb80d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = os.environ.get(\"AVATAR_BASE_URL\")\n",
    "username = os.environ.get(\"AVATAR_USERNAME\")\n",
    "password = os.environ.get(\"AVATAR_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the client that you'll be using for all of your requests\n",
    "from avatars.client import ApiClient\n",
    "from avatars.models import (\n",
    "    AvatarizationJobCreate,\n",
    "    AvatarizationParameters,\n",
    "    ImputationParameters,\n",
    "    ImputeMethod,\n",
    ")\n",
    "from avatars.models import ReportCreate\n",
    "\n",
    "from avatars.api import AvatarizationPipelineCreate\n",
    "from avatars.processors import ProportionProcessor\n",
    "from avatars.processors import GroupModalitiesProcessor\n",
    "from avatars.processors import RelativeDifferenceProcessor\n",
    "from avatars.processors import PerturbationProcessor\n",
    "from avatars.processors import ExpectedMeanProcessor\n",
    "\n",
    "# The following are not necessary to run avatar but are used in this tutorial\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "\n",
    "# Change this to your actual server endpoint, e.g. base_url=\"https://avatar.company.com\"\n",
    "client = ApiClient(base_url=url)\n",
    "client.authenticate(username=username, password=password)\n",
    "\n",
    "# Verify that we can connect to the API server\n",
    "client.health.get_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9d732",
   "metadata": {},
   "source": [
    "## Load some time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04336b1a",
   "metadata": {},
   "source": [
    "We will use an example dataset that contains data on 2 sensors for 50 devices. For each device, 100 time points are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ac9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../fixtures/sensors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e53b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    df, x=\"t\", y=\"sensor1\", hue=\"id\", palette=sns.color_palette(), legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a097ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    df, x=\"t\", y=\"sensor2\", hue=\"id\", palette=sns.color_palette(), legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd0de8",
   "metadata": {},
   "source": [
    "## Prepare data for avatarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff3f6c",
   "metadata": {},
   "source": [
    "The avatarization takes as input tabular data where each row contains the data relative to an individual. In the present example, each row should ideally refer to a device.\n",
    "\n",
    "The number of time points to include in the avatarization can also have an impact and it is currently recommended to use a small number of data points (~ 5 to 10 points) to prevent cases where the data has more variables than individuals. \n",
    "\n",
    "To perform the transformation which consists in pivotting the table and sampling a given number of time points, we will use a processor.\n",
    "\n",
    "We can call this processor `PivotTimeSeriesProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PivotTimeSeriesProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_points: int,\n",
    "        id_variable: str,\n",
    "        time_variable: str,\n",
    "        values_variables: List[str],\n",
    "    ):\n",
    "        self.n_points = n_points\n",
    "        self.id_variable = id_variable\n",
    "        self.time_variable = time_variable\n",
    "        self.values_variables = values_variables\n",
    "        self.fixed_variables = None\n",
    "        self.times = None\n",
    "\n",
    "    def get_sampled_data_for_id(self, df, the_id, to_convert):\n",
    "        df_individual = df[df[self.id_variable] == the_id]\n",
    "\n",
    "        idx = np.round(np.linspace(0, len(df_individual) - 1, self.n_points)).astype(\n",
    "            int\n",
    "        )  # indices to sample\n",
    "        df_sampled = df_individual.iloc[idx].reset_index(drop=True)  # sampling\n",
    "\n",
    "        data = {self.id_variable: [the_id]}\n",
    "\n",
    "        # Create one variable per pair {variable to convert - time step sampled}\n",
    "        for c in to_convert:\n",
    "            for i in df_sampled.index:\n",
    "                data[c + \"_\" + str(i)] = [df_sampled.loc[i, c]]\n",
    "\n",
    "        # For fixed variables, only create one variable and take the first value\n",
    "        for c in self.fixed_variables:\n",
    "            data[c] = [df_sampled.loc[0, c]]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        dfs = []  # list of df, one per ID\n",
    "        to_convert = self.values_variables.copy()\n",
    "        to_convert.append(self.time_variable)\n",
    "        self.fixed_variables = [\n",
    "            c for c in df.columns if c not in to_convert and c != self.id_variable\n",
    "        ]\n",
    "\n",
    "        # For each ID, sample points evenly spaced\n",
    "        for the_id in np.unique(df[self.id_variable]):\n",
    "            data = self.get_sampled_data_for_id(df, the_id, to_convert)\n",
    "            dfs.append(pd.DataFrame(data))\n",
    "\n",
    "        working = pd.concat(dfs)\n",
    "        working = working.reset_index(drop=True)\n",
    "\n",
    "        # Save time columns (constant across all individuals) and remove them from preprocessed df\n",
    "        time_columns = [self.time_variable + \"_\" + str(i) for i in range(self.n_points)]\n",
    "        self.times = working[time_columns]\n",
    "        working = working.drop(columns=time_columns)\n",
    "        working = working.drop(columns=[self.id_variable])\n",
    "\n",
    "        return working\n",
    "\n",
    "    def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:\n",
    "        working = dest.copy()\n",
    "\n",
    "        # Add back the saved time information\n",
    "        working = pd.concat([working, self.times], axis=1)\n",
    "\n",
    "        # Transform time series variables\n",
    "        data = {}\n",
    "        for variable in self.values_variables:\n",
    "            data[variable] = []\n",
    "            for the_id in range(len(working)):\n",
    "                for i in range(self.n_points):\n",
    "                    data[variable].append(working.loc[the_id, variable + \"_\" + str(i)])\n",
    "\n",
    "        # Transform time and ID variables\n",
    "        data[self.time_variable] = []\n",
    "        data[self.id_variable] = []\n",
    "        for the_id in range(len(working)):\n",
    "            for i in range(self.n_points):\n",
    "                data[self.time_variable].append(\n",
    "                    working.loc[the_id, self.time_variable + \"_\" + str(i)]\n",
    "                )\n",
    "                data[self.id_variable].append(the_id)\n",
    "\n",
    "        # Transform fixed variables\n",
    "        for c in self.fixed_variables:\n",
    "            data[c] = []\n",
    "            for the_id in range(len(working)):\n",
    "                data[c].extend([working.loc[the_id, c] for _ in range(self.n_points)])\n",
    "\n",
    "        postprocessed = pd.DataFrame(data)\n",
    "        return postprocessed[df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf26a13",
   "metadata": {},
   "source": [
    "We can check that the behavior of the processor is as expected: after preprocessing, we should have *n_points* variables for each sensor and the static variables (i.e. the *model* variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_processor = PivotTimeSeriesProcessor(\n",
    "    n_points=10,\n",
    "    id_variable=\"id\",\n",
    "    time_variable=\"t\",\n",
    "    values_variables=[\"sensor1\", \"sensor2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = timeseries_processor.preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309b406",
   "metadata": {},
   "source": [
    "The post-processing step should reformat the data from the pre-processed form into the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessed_df = timeseries_processor.postprocess(df, preprocessed_df)\n",
    "postprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb990186",
   "metadata": {},
   "source": [
    "## Avatarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34677e7",
   "metadata": {},
   "source": [
    "The data is now in a classic tabular format and can be avatarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.pandas_integration.upload_dataframe(preprocessed_df)\n",
    "print(preprocessed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.jobs.create_full_avatarization_job(\n",
    "    AvatarizationJobCreate(\n",
    "        parameters=AvatarizationParameters(k=5, dataset_id=dataset.id)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.jobs.get_avatarization_job(id=job.id, timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.id)\n",
    "print(job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatars_df = client.pandas_integration.download_dataframe(job.result.avatars_dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11fd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_metrics = job.result.privacy_metrics\n",
    "print(\"*** Privacy metrics ***\")\n",
    "for metric in privacy_metrics:\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c3ee7",
   "metadata": {},
   "source": [
    "### Conversion of avatars back in original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_postprocessed_df = timeseries_processor.postprocess(df, avatars_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_postprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a14083",
   "metadata": {},
   "source": [
    "### Comparing original and avatarized time series\n",
    "\n",
    "Note that because the post-process step of our processor does not perform interpolation, we only plot the sampled original data and the avatars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_originals = timeseries_processor.postprocess(df, preprocessed_df)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "\n",
    "for ax, df, suptitle in zip(\n",
    "    axs,\n",
    "    [sampled_originals, avatar_postprocessed_df],\n",
    "    [\"Sampled original\", \"Sampled avatars\"],\n",
    "):\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=df,\n",
    "        x=\"t\",\n",
    "        y=\"sensor1\",\n",
    "        hue=\"id\",\n",
    "        palette=sns.color_palette(),\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(suptitle)\n",
    "\n",
    "fig.suptitle(\"Comparison of sensor1 data\", fontsize=20)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "\n",
    "for ax, df, suptitle in zip(\n",
    "    axs,\n",
    "    [sampled_originals, avatar_postprocessed_df],\n",
    "    [\"Sampled original\", \"Sampled avatars\"],\n",
    "):\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=df,\n",
    "        x=\"t\",\n",
    "        y=\"sensor2\",\n",
    "        hue=\"id\",\n",
    "        palette=sns.color_palette(),\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(suptitle)\n",
    "fig.suptitle(\"Comparison of sensor2 data\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbfb95",
   "metadata": {},
   "source": [
    "*In the next tutorial, we will show how to perform the avatarization on data batches*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
